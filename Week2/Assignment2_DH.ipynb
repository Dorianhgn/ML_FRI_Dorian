{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment nÂ°2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from matplotlib.pyplot import subplots \n",
    "from plotnine import *\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS, summarize, poly)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from functools import partial\n",
    "from sklearn.model_selection import ( \\\n",
    "    cross_validate,\n",
    "    KFold,\n",
    "    ShuffleSplit)\n",
    "from sklearn.base import clone\n",
    "from ISLP.models import sklearn_sm\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 183, 'name': 'Communities and Crime', 'repository_url': 'https://archive.ics.uci.edu/dataset/183/communities+and+crime', 'data_url': 'https://archive.ics.uci.edu/static/public/183/data.csv', 'abstract': 'Communities within the United States. The data combines socio-economic data from the 1990 US Census, law enforcement data from the 1990 US LEMAS survey, and crime data from the 1995 FBI UCR.', 'area': 'Social Science', 'tasks': ['Regression'], 'characteristics': ['Multivariate'], 'num_instances': 1994, 'num_features': 127, 'feature_types': ['Real'], 'demographics': ['Race', 'Age', 'Income', 'Occupation'], 'target_col': ['ViolentCrimesPerPop'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 2002, 'last_updated': 'Mon Mar 04 2024', 'dataset_doi': '10.24432/C53W3X', 'creators': ['Michael Redmond'], 'intro_paper': {'ID': 405, 'type': 'NATIVE', 'title': 'A data-driven software tool for enabling cooperative information sharing among police departments', 'authors': 'Michael Redmond, Alok Baveja', 'venue': 'European Journal of Operational Research', 'year': 2002, 'journal': None, 'DOI': '10.1016/S0377-2217(01)00264-8', 'URL': 'https://www.semanticscholar.org/paper/2cbe83cae40f5236146aae99522397407a56e7d7', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': \"  Many variables are included so that algorithms that select or learn weights for attributes could be tested. However, clearly unrelated attributes were not included; attributes were picked if there was any plausible connection to crime (N=122), plus the attribute to be predicted (Per Capita Violent Crimes). The variables included in the dataset involve the community, such as the percent of the population considered urban, and the median family income, and involving law enforcement, such as per capita number of police officers, and percent of officers assigned to drug units. \\r\\n \\r\\n The per capita violent crimes variable was calculated using population and the sum of crime variables considered violent crimes in the United States: murder, rape, robbery, and assault. There was apparently some controversy in some states concerning the counting of rapes. These resulted in missing values for rape, which resulted in incorrect values for per capita violent crime. These cities are not included in the dataset. Many of these omitted communities were from the midwestern USA.\\r\\n\\r\\n  Data is described below based on original values. All numeric data was normalized into the decimal range  0.00-1.00 using an Unsupervised, equal-interval binning method. Attributes retain their distribution and skew (hence for example the population attribute has a mean value of  0.06 because most communities are small). E.g. An attribute described as 'mean people per household' is actually the normalized (0-1) version of that value.\\r\\n\\r\\n  The normalization preserves rough ratios of values WITHIN an attribute (e.g. double the value for double the population within the available precision - except for extreme values (all values more than 3 SD above the mean are normalized to 1.00; all values more than 3 SD below the mean are nromalized to  0.00)).\\r\\n\\r\\n  However, the normalization does not preserve relationships between values BETWEEN attributes (e.g. it would not be meaningful to compare the value for whitePerCap with the value for blackPerCap for a community)\\r\\n\\r\\n  A limitation was that the LEMAS survey was of the police departments with at least 100 officers, plus a random sample of smaller departments. For our purposes, communities not found in both census and crime datasets were omitted. Many communities are missing LEMAS data.\\r\\n\\r\\n.arff header for Weka:\\r\\n\\r\\n@relation crimepredict\\r\\n\\r\\n@attribute state numeric\\r\\n@attribute county numeric\\r\\n@attribute community numeric\\r\\n@attribute communityname string\\r\\n@attribute fold numeric\\r\\n@attribute population numeric\\r\\n@attribute householdsize numeric\\r\\n@attribute racepctblack numeric\\r\\n@attribute racePctWhite numeric\\r\\n@attribute racePctAsian numeric\\r\\n@attribute racePctHisp numeric\\r\\n@attribute agePct12t21 numeric\\r\\n@attribute agePct12t29 numeric\\r\\n@attribute agePct16t24 numeric\\r\\n@attribute agePct65up numeric\\r\\n@attribute numbUrban numeric\\r\\n@attribute pctUrban numeric\\r\\n@attribute medIncome numeric\\r\\n@attribute pctWWage numeric\\r\\n@attribute pctWFarmSelf numeric\\r\\n@attribute pctWInvInc numeric\\r\\n@attribute pctWSocSec numeric\\r\\n@attribute pctWPubAsst numeric\\r\\n@attribute pctWRetire numeric\\r\\n@attribute medFamInc numeric\\r\\n@attribute perCapInc numeric\\r\\n@attribute whitePerCap numeric\\r\\n@attribute blackPerCap numeric\\r\\n@attribute indianPerCap numeric\\r\\n@attribute AsianPerCap numeric\\r\\n@attribute OtherPerCap numeric\\r\\n@attribute HispPerCap numeric\\r\\n@attribute NumUnderPov numeric\\r\\n@attribute PctPopUnderPov numeric\\r\\n@attribute PctLess9thGrade numeric\\r\\n@attribute PctNotHSGrad numeric\\r\\n@attribute PctBSorMore numeric\\r\\n@attribute PctUnemployed numeric\\r\\n@attribute PctEmploy numeric\\r\\n@attribute PctEmplManu numeric\\r\\n@attribute PctEmplProfServ numeric\\r\\n@attribute PctOccupManu numeric\\r\\n@attribute PctOccupMgmtProf numeric\\r\\n@attribute MalePctDivorce numeric\\r\\n@attribute MalePctNevMarr numeric\\r\\n@attribute FemalePctDiv numeric\\r\\n@attribute TotalPctDiv numeric\\r\\n@attribute PersPerFam numeric\\r\\n@attribute PctFam2Par numeric\\r\\n@attribute PctKids2Par numeric\\r\\n@attribute PctYoungKids2Par numeric\\r\\n@attribute PctTeen2Par numeric\\r\\n@attribute PctWorkMomYoungKids numeric\\r\\n@attribute PctWorkMom numeric\\r\\n@attribute NumIlleg numeric\\r\\n@attribute PctIlleg numeric\\r\\n@attribute NumImmig numeric\\r\\n@attribute PctImmigRecent numeric\\r\\n@attribute PctImmigRec5 numeric\\r\\n@attribute PctImmigRec8 numeric\\r\\n@attribute PctImmigRec10 numeric\\r\\n@attribute PctRecentImmig numeric\\r\\n@attribute PctRecImmig5 numeric\\r\\n@attribute PctRecImmig8 numeric\\r\\n@attribute PctRecImmig10 numeric\\r\\n@attribute PctSpeakEnglOnly numeric\\r\\n@attribute PctNotSpeakEnglWell numeric\\r\\n@attribute PctLargHouseFam numeric\\r\\n@attribute PctLargHouseOccup numeric\\r\\n@attribute PersPerOccupHous numeric\\r\\n@attribute PersPerOwnOccHous numeric\\r\\n@attribute PersPerRentOccHous numeric\\r\\n@attribute PctPersOwnOccup numeric\\r\\n@attribute PctPersDenseHous numeric\\r\\n@attribute PctHousLess3BR numeric\\r\\n@attribute MedNumBR numeric\\r\\n@attribute HousVacant numeric\\r\\n@attribute PctHousOccup numeric\\r\\n@attribute PctHousOwnOcc numeric\\r\\n@attribute PctVacantBoarded numeric\\r\\n@attribute PctVacMore6Mos numeric\\r\\n@attribute MedYrHousBuilt numeric\\r\\n@attribute PctHousNoPhone numeric\\r\\n@attribute PctWOFullPlumb numeric\\r\\n@attribute OwnOccLowQuart numeric\\r\\n@attribute OwnOccMedVal numeric\\r\\n@attribute OwnOccHiQuart numeric\\r\\n@attribute RentLowQ numeric\\r\\n@attribute RentMedian numeric\\r\\n@attribute RentHighQ numeric\\r\\n@attribute MedRent numeric\\r\\n@attribute MedRentPctHousInc numeric\\r\\n@attribute MedOwnCostPctInc numeric\\r\\n@attribute MedOwnCostPctIncNoMtg numeric\\r\\n@attribute NumInShelters numeric\\r\\n@attribute NumStreet numeric\\r\\n@attribute PctForeignBorn numeric\\r\\n@attribute PctBornSameState numeric\\r\\n@attribute PctSameHouse85 numeric\\r\\n@attribute PctSameCity85 numeric\\r\\n@attribute PctSameState85 numeric\\r\\n@attribute LemasSwornFT numeric\\r\\n@attribute LemasSwFTPerPop numeric\\r\\n@attribute LemasSwFTFieldOps numeric\\r\\n@attribute LemasSwFTFieldPerPop numeric\\r\\n@attribute LemasTotalReq numeric\\r\\n@attribute LemasTotReqPerPop numeric\\r\\n@attribute PolicReqPerOffic numeric\\r\\n@attribute PolicPerPop numeric\\r\\n@attribute RacialMatchCommPol numeric\\r\\n@attribute PctPolicWhite numeric\\r\\n@attribute PctPolicBlack numeric\\r\\n@attribute PctPolicHisp numeric\\r\\n@attribute PctPolicAsian numeric\\r\\n@attribute PctPolicMinor numeric\\r\\n@attribute OfficAssgnDrugUnits numeric\\r\\n@attribute NumKindsDrugsSeiz numeric\\r\\n@attribute PolicAveOTWorked numeric\\r\\n@attribute LandArea numeric\\r\\n@attribute PopDens numeric\\r\\n@attribute PctUsePubTrans numeric\\r\\n@attribute PolicCars numeric\\r\\n@attribute PolicOperBudg numeric\\r\\n@attribute LemasPctPolicOnPatr numeric\\r\\n@attribute LemasGangUnitDeploy numeric\\r\\n@attribute LemasPctOfficDrugUn numeric\\r\\n@attribute PolicBudgPerPop numeric\\r\\n@attribute ViolentCrimesPerPop numeric\\r\\n\\r\\n@data\\r\\n\\r\\n  \", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': \"Attribute Information: (122 predictive, 5 non-predictive, 1 goal)\\r\\n  -- state: US state (by number) - not counted as predictive above, but if considered, should be consided nominal (nominal) \\r\\n  -- county: numeric code for county - not predictive, and many missing values (numeric)\\r\\n  -- community: numeric code for community - not predictive and many missing values (numeric)\\r\\n  -- communityname: community name - not predictive - for information only (string)\\r\\n  -- fold: fold number for non-random 10 fold cross validation, potentially useful for debugging, paired tests - not predictive (numeric)\\r\\n  -- population: population for community: (numeric - decimal)\\r\\n  -- householdsize: mean people per household (numeric - decimal)\\r\\n  -- racepctblack: percentage of population that is african american (numeric - decimal)\\r\\n  -- racePctWhite: percentage of population that is caucasian (numeric - decimal)\\r\\n  -- racePctAsian: percentage of population that is of asian heritage (numeric - decimal)\\r\\n  -- racePctHisp: percentage of population that is of hispanic heritage (numeric - decimal)\\r\\n  -- agePct12t21: percentage of population that is 12-21 in age (numeric - decimal)\\r\\n  -- agePct12t29: percentage of population that is 12-29 in age (numeric - decimal)\\r\\n  -- agePct16t24: percentage of population that is 16-24 in age (numeric - decimal)\\r\\n  -- agePct65up: percentage of population that is 65 and over in age (numeric - decimal)\\r\\n  -- numbUrban: number of people living in areas classified as urban (numeric - decimal)\\r\\n  -- pctUrban: percentage of people living in areas classified as urban (numeric - decimal)\\r\\n  -- medIncome: median household income (numeric - decimal)\\r\\n  -- pctWWage: percentage of households with wage or salary income in 1989 (numeric - decimal)\\r\\n  -- pctWFarmSelf: percentage of households with farm or self employment income in 1989 (numeric - decimal)\\r\\n  -- pctWInvInc: percentage of households with investment / rent income in 1989 (numeric - decimal)\\r\\n  -- pctWSocSec: percentage of households with social security income in 1989 (numeric - decimal)\\r\\n  -- pctWPubAsst: percentage of households with public assistance income in 1989 (numeric - decimal)\\r\\n  -- pctWRetire: percentage of households with retirement income in 1989 (numeric - decimal)\\r\\n  -- medFamInc: median family income (differs from household income for non-family households) (numeric - decimal)\\r\\n  -- perCapInc: per capita income (numeric - decimal)\\r\\n  -- whitePerCap: per capita income for caucasians (numeric - decimal)\\r\\n  -- blackPerCap: per capita income for african americans (numeric - decimal)\\r\\n  -- indianPerCap: per capita income for native americans (numeric - decimal)\\r\\n  -- AsianPerCap: per capita income for people with asian heritage (numeric - decimal)\\r\\n  -- OtherPerCap: per capita income for people with 'other' heritage (numeric - decimal)\\r\\n  -- HispPerCap: per capita income for people with hispanic heritage (numeric - decimal)\\r\\n  -- NumUnderPov: number of people under the poverty level (numeric - decimal)\\r\\n  -- PctPopUnderPov: percentage of people under the poverty level (numeric - decimal)\\r\\n  -- PctLess9thGrade: percentage of people 25 and over with less than a 9th grade education (numeric - decimal)\\r\\n  -- PctNotHSGrad: percentage of people 25 and over that are not high school graduates (numeric - decimal)\\r\\n  -- PctBSorMore: percentage of people 25 and over with a bachelors degree or higher education (numeric - decimal)\\r\\n  -- PctUnemployed: percentage of people 16 and over, in the labor force, and unemployed (numeric - decimal)\\r\\n  -- PctEmploy: percentage of people 16 and over who are employed (numeric - decimal)\\r\\n  -- PctEmplManu: percentage of people 16 and over who are employed in manufacturing (numeric - decimal)\\r\\n  -- PctEmplProfServ: percentage of people 16 and over who are employed in professional services (numeric - decimal)\\r\\n  -- PctOccupManu: percentage of people 16 and over who are employed in manufacturing (numeric - decimal)   ########\\r\\n  -- PctOccupMgmtProf: percentage of people 16 and over who are employed in management or professional occupations (numeric - decimal)\\r\\n  -- MalePctDivorce: percentage of males who are divorced (numeric - decimal)\\r\\n  -- MalePctNevMarr: percentage of males who have never married (numeric - decimal)\\r\\n  -- FemalePctDiv: percentage of females who are divorced (numeric - decimal)\\r\\n  -- TotalPctDiv: percentage of population who are divorced (numeric - decimal)\\r\\n  -- PersPerFam: mean number of people per family (numeric - decimal)\\r\\n  -- PctFam2Par: percentage of families (with kids) that are headed by two parents (numeric - decimal)\\r\\n  -- PctKids2Par: percentage of kids in family housing with two parents (numeric - decimal)\\r\\n  -- PctYoungKids2Par: percent of kids 4 and under in two parent households (numeric - decimal)\\r\\n  -- PctTeen2Par: percent of kids age 12-17 in two parent households (numeric - decimal)\\r\\n  -- PctWorkMomYoungKids: percentage of moms of kids 6 and under in labor force (numeric - decimal)\\r\\n  -- PctWorkMom: percentage of moms of kids under 18 in labor force (numeric - decimal)\\r\\n  -- NumIlleg: number of kids born to never married (numeric - decimal)\\r\\n  -- PctIlleg: percentage of kids born to never married (numeric - decimal)\\r\\n  -- NumImmig: total number of people known to be foreign born (numeric - decimal)\\r\\n  -- PctImmigRecent: percentage of _immigrants_ who immigated within last 3 years (numeric - decimal)\\r\\n  -- PctImmigRec5: percentage of _immigrants_ who immigated within last 5 years (numeric - decimal)\\r\\n  -- PctImmigRec8: percentage of _immigrants_ who immigated within last 8 years (numeric - decimal)\\r\\n  -- PctImmigRec10: percentage of _immigrants_ who immigated within last 10 years (numeric - decimal)\\r\\n  -- PctRecentImmig: percent of _population_ who have immigrated within the last 3 years (numeric - decimal)\\r\\n  -- PctRecImmig5: percent of _population_ who have immigrated within the last 5 years (numeric - decimal)\\r\\n  -- PctRecImmig8: percent of _population_ who have immigrated within the last 8 years (numeric - decimal)\\r\\n  -- PctRecImmig10: percent of _population_ who have immigrated within the last 10 years (numeric - decimal)\\r\\n  -- PctSpeakEnglOnly: percent of people who speak only English (numeric - decimal)\\r\\n  -- PctNotSpeakEnglWell: percent of people who do not speak English well (numeric - decimal)\\r\\n  -- PctLargHouseFam: percent of family households that are large (6 or more) (numeric - decimal)\\r\\n  -- PctLargHouseOccup: percent of all occupied households that are large (6 or more people) (numeric - decimal)\\r\\n  -- PersPerOccupHous: mean persons per household (numeric - decimal)\\r\\n  -- PersPerOwnOccHous: mean persons per owner occupied household (numeric - decimal)\\r\\n  -- PersPerRentOccHous: mean persons per rental household (numeric - decimal)\\r\\n  -- PctPersOwnOccup: percent of people in owner occupied households (numeric - decimal)\\r\\n  -- PctPersDenseHous: percent of persons in dense housing (more than 1 person per room) (numeric - decimal)\\r\\n  -- PctHousLess3BR: percent of housing units with less than 3 bedrooms (numeric - decimal)\\r\\n  -- MedNumBR: median number of bedrooms (numeric - decimal)\\r\\n  -- HousVacant: number of vacant households (numeric - decimal)\\r\\n  -- PctHousOccup: percent of housing occupied (numeric - decimal)\\r\\n  -- PctHousOwnOcc: percent of households owner occupied (numeric - decimal)\\r\\n  -- PctVacantBoarded: percent of vacant housing that is boarded up (numeric - decimal)\\r\\n  -- PctVacMore6Mos: percent of vacant housing that has been vacant more than 6 months (numeric - decimal)\\r\\n  -- MedYrHousBuilt: median year housing units built (numeric - decimal)\\r\\n  -- PctHousNoPhone: percent of occupied housing units without phone (in 1990, this was rare!) (numeric - decimal)\\r\\n  -- PctWOFullPlumb: percent of housing without complete plumbing facilities (numeric - decimal)\\r\\n  -- OwnOccLowQuart: owner occupied housing - lower quartile value (numeric - decimal)\\r\\n  -- OwnOccMedVal: owner occupied housing - median value (numeric - decimal)\\r\\n  -- OwnOccHiQuart: owner occupied housing - upper quartile value (numeric - decimal)\\r\\n  -- RentLowQ: rental housing - lower quartile rent (numeric - decimal)\\r\\n  -- RentMedian: rental housing - median rent (Census variable H32B from file STF1A) (numeric - decimal)\\r\\n  -- RentHighQ: rental housing - upper quartile rent (numeric - decimal)\\r\\n  -- MedRent: median gross rent (Census variable H43A from file STF3A - includes utilities) (numeric - decimal)\\r\\n  -- MedRentPctHousInc: median gross rent as a percentage of household income (numeric - decimal)\\r\\n  -- MedOwnCostPctInc: median owners cost as a percentage of household income - for owners with a mortgage (numeric - decimal)\\r\\n  -- MedOwnCostPctIncNoMtg: median owners cost as a percentage of household income - for owners without a mortgage (numeric - decimal)\\r\\n  -- NumInShelters: number of people in homeless shelters (numeric - decimal)\\r\\n  -- NumStreet: number of homeless people counted in the street (numeric - decimal)\\r\\n  -- PctForeignBorn: percent of people foreign born (numeric - decimal)\\r\\n  -- PctBornSameState: percent of people born in the same state as currently living (numeric - decimal)\\r\\n  -- PctSameHouse85: percent of people living in the same house as in 1985 (5 years before) (numeric - decimal)\\r\\n  -- PctSameCity85: percent of people living in the same city as in 1985 (5 years before) (numeric - decimal)\\r\\n  -- PctSameState85: percent of people living in the same state as in 1985 (5 years before) (numeric - decimal)\\r\\n  -- LemasSwornFT: number of sworn full time police officers (numeric - decimal)\\r\\n  -- LemasSwFTPerPop: sworn full time police officers per 100K population (numeric - decimal)\\r\\n  -- LemasSwFTFieldOps: number of sworn full time police officers in field operations (on the street as opposed to administrative etc) (numeric - decimal)\\r\\n  -- LemasSwFTFieldPerPop: sworn full time police officers in field operations (on the street as opposed to administrative etc) per 100K population (numeric - decimal)\\r\\n  -- LemasTotalReq: total requests for police (numeric - decimal)\\r\\n  -- LemasTotReqPerPop: total requests for police per 100K popuation (numeric - decimal)\\r\\n  -- PolicReqPerOffic: total requests for police per police officer (numeric - decimal)\\r\\n  -- PolicPerPop: police officers per 100K population (numeric - decimal)\\r\\n  -- RacialMatchCommPol: a measure of the racial match between the community and the police force. High values indicate proportions in community and police force are similar (numeric - decimal)\\r\\n  -- PctPolicWhite: percent of police that are caucasian (numeric - decimal)\\r\\n  -- PctPolicBlack: percent of police that are african american (numeric - decimal)\\r\\n  -- PctPolicHisp: percent of police that are hispanic (numeric - decimal)\\r\\n  -- PctPolicAsian: percent of police that are asian (numeric - decimal)\\r\\n  -- PctPolicMinor: percent of police that are minority of any kind (numeric - decimal)\\r\\n  -- OfficAssgnDrugUnits: number of officers assigned to special drug units (numeric - decimal)\\r\\n  -- NumKindsDrugsSeiz: number of different kinds of drugs seized (numeric - decimal)\\r\\n  -- PolicAveOTWorked: police average overtime worked (numeric - decimal)\\r\\n  -- LandArea: land area in square miles (numeric - decimal)\\r\\n  -- PopDens: population density in persons per square mile (numeric - decimal)\\r\\n  -- PctUsePubTrans: percent of people using public transit for commuting (numeric - decimal)\\r\\n  -- PolicCars: number of police cars (numeric - decimal)\\r\\n  -- PolicOperBudg: police operating budget (numeric - decimal)\\r\\n  -- LemasPctPolicOnPatr: percent of sworn full time police officers on patrol (numeric - decimal)\\r\\n  -- LemasGangUnitDeploy: gang unit deployed (numeric - decimal - but really ordinal - 0 means NO, 1 means YES,  0.5 means Part Time)\\r\\n  -- LemasPctOfficDrugUn: percent of officers assigned to drug units (numeric - decimal)\\r\\n  -- PolicBudgPerPop: police operating budget per population (numeric - decimal)\\r\\n  -- ViolentCrimesPerPop: total number of violent crimes per 100K popuation (numeric - decimal) GOAL attribute (to be predicted)\\r\\n\\r\\nSummary Statistics:\\r\\n\\t\\t\\tMin\\tMax\\t Mean\\t SD\\tCorrel\\tMedian\\t Mode\\tMissing\\r\\npopulation\\t\\t0\\t1\\t 0.06\\t 0.13\\t 0.37\\t 0.02\\t 0.01\\t0\\r\\nhouseholdsize\\t\\t0\\t1\\t 0.46\\t 0.16\\t-0.03\\t 0.44\\t 0.41\\t0\\r\\nracepctblack\\t\\t0\\t1\\t 0.18\\t 0.25\\t 0.63\\t 0.06\\t 0.01\\t0\\r\\nracePctWhite\\t\\t0\\t1\\t 0.75\\t 0.24\\t-0.68\\t 0.85\\t 0.98\\t0\\r\\nracePctAsian\\t\\t0\\t1\\t 0.15\\t 0.21\\t 0.04\\t 0.07\\t 0.02\\t0\\r\\nracePctHisp\\t\\t0\\t1\\t 0.14\\t 0.23\\t 0.29\\t 0.04\\t 0.01\\t0\\r\\nagePct12t21\\t\\t0\\t1\\t 0.42\\t 0.16\\t 0.06\\t 0.4\\t 0.38\\t0\\r\\nagePct12t29\\t\\t0\\t1\\t 0.49\\t 0.14\\t 0.15\\t 0.48\\t 0.49\\t0\\r\\nagePct16t24\\t\\t0\\t1\\t 0.34\\t 0.17\\t 0.10\\t 0.29\\t 0.29\\t0\\r\\nagePct65up\\t\\t0\\t1\\t 0.42\\t 0.18\\t 0.07\\t 0.42\\t 0.47\\t0\\r\\nnumbUrban\\t\\t0\\t1\\t 0.06\\t 0.13\\t 0.36\\t 0.03\\t 0\\t0\\r\\npctUrban\\t\\t0\\t1\\t 0.70\\t 0.44\\t 0.08\\t 1\\t 1\\t0\\r\\nmedIncome\\t\\t0\\t1\\t 0.36\\t 0.21\\t-0.42\\t 0.32\\t 0.23\\t0\\r\\npctWWage\\t\\t0\\t1\\t 0.56\\t 0.18\\t-0.31\\t 0.56\\t 0.58\\t0\\r\\npctWFarmSelf\\t\\t0\\t1\\t 0.29\\t 0.20\\t-0.15\\t 0.23\\t 0.16\\t0\\r\\npctWInvInc\\t\\t0\\t1\\t 0.50\\t 0.18\\t-0.58\\t 0.48\\t 0.41\\t0\\r\\npctWSocSec\\t\\t0\\t1\\t 0.47\\t 0.17\\t 0.12\\t 0.475\\t 0.56\\t0\\r\\npctWPubAsst\\t\\t0\\t1\\t 0.32\\t 0.22\\t 0.57\\t 0.26\\t 0.1\\t0\\r\\npctWRetire\\t\\t0\\t1\\t 0.48\\t 0.17\\t-0.10\\t 0.47\\t 0.44\\t0\\r\\nmedFamInc\\t\\t0\\t1\\t 0.38\\t 0.20\\t-0.44\\t 0.33\\t 0.25\\t0\\r\\nperCapInc\\t\\t0\\t1\\t 0.35\\t 0.19\\t-0.35\\t 0.3\\t 0.23\\t0\\r\\nwhitePerCap\\t\\t0\\t1\\t 0.37\\t 0.19\\t-0.21\\t 0.32\\t 0.3\\t0\\r\\nblackPerCap\\t\\t0\\t1\\t 0.29\\t 0.17\\t-0.28\\t 0.25\\t 0.18\\t0\\r\\nindianPerCap\\t\\t0\\t1\\t 0.20\\t 0.16\\t-0.09\\t 0.17\\t 0\\t0\\r\\nAsianPerCap\\t\\t0\\t1\\t 0.32\\t 0.20\\t-0.16\\t 0.28\\t 0.18\\t0\\r\\nOtherPerCap\\t\\t0\\t1\\t 0.28\\t 0.19\\t-0.13\\t 0.25\\t 0\\t1\\r\\nHispPerCap\\t\\t0\\t1\\t 0.39\\t 0.18\\t-0.24\\t 0.345\\t 0.3\\t0\\r\\nNumUnderPov\\t\\t0\\t1\\t 0.06\\t 0.13\\t 0.45\\t 0.02\\t 0.01\\t0\\r\\nPctPopUnderPov\\t\\t0\\t1\\t 0.30\\t 0.23\\t 0.52\\t 0.25\\t 0.08\\t0\\r\\nPctLess9thGrade\\t\\t0\\t1\\t 0.32\\t 0.21\\t 0.41\\t 0.27\\t 0.19\\t0\\r\\nPctNotHSGrad\\t\\t0\\t1\\t 0.38\\t 0.20\\t 0.48\\t 0.36\\t 0.39\\t0\\r\\nPctBSorMore\\t\\t0\\t1\\t 0.36\\t 0.21\\t-0.31\\t 0.31\\t 0.18\\t0\\r\\nPctUnemployed\\t\\t0\\t1\\t 0.36\\t 0.20\\t 0.50\\t 0.32\\t 0.24\\t0\\r\\nPctEmploy\\t\\t0\\t1\\t 0.50\\t 0.17\\t-0.33\\t 0.51\\t 0.56\\t0\\r\\nPctEmplManu\\t\\t0\\t1\\t 0.40\\t 0.20\\t-0.04\\t 0.37\\t 0.26\\t0\\r\\nPctEmplProfServ\\t\\t0\\t1\\t 0.44\\t 0.18\\t-0.07\\t 0.41\\t 0.36\\t0\\r\\nPctOccupManu\\t\\t0\\t1\\t 0.39\\t 0.20\\t 0.30\\t 0.37\\t 0.32\\t0\\r\\nPctOccupMgmtProf\\t0\\t1\\t 0.44\\t 0.19\\t-0.34\\t 0.4\\t 0.36\\t0\\r\\nMalePctDivorce\\t\\t0\\t1\\t 0.46\\t 0.18\\t 0.53\\t 0.47\\t 0.56\\t0\\r\\nMalePctNevMarr\\t\\t0\\t1\\t 0.43\\t 0.18\\t 0.30\\t 0.4\\t 0.38\\t0\\r\\nFemalePctDiv\\t\\t0\\t1\\t 0.49\\t 0.18\\t 0.56\\t 0.5\\t 0.54\\t0\\r\\nTotalPctDiv\\t\\t0\\t1\\t 0.49\\t 0.18\\t 0.55\\t 0.5\\t 0.57\\t0\\r\\nPersPerFam\\t\\t0\\t1\\t 0.49\\t 0.15\\t 0.14\\t 0.47\\t 0.44\\t0\\r\\nPctFam2Par\\t\\t0\\t1\\t 0.61\\t 0.20\\t-0.71\\t 0.63\\t 0.7\\t0\\r\\nPctKids2Par\\t\\t0\\t1\\t 0.62\\t 0.21\\t-0.74\\t 0.64\\t 0.72\\t0\\r\\nPctYoungKids2Par\\t0\\t1\\t 0.66\\t 0.22\\t-0.67\\t 0.7\\t 0.91\\t0\\r\\nPctTeen2Par\\t\\t0\\t1\\t 0.58\\t 0.19\\t-0.66\\t 0.61\\t 0.6\\t0\\r\\nPctWorkMomYoungKids\\t0\\t1\\t 0.50\\t 0.17\\t-0.02\\t 0.51\\t 0.51\\t0\\r\\nPctWorkMom\\t\\t0\\t1\\t 0.53\\t 0.18\\t-0.15\\t 0.54\\t 0.57\\t0\\r\\nNumIlleg\\t\\t0\\t1\\t 0.04\\t 0.11\\t 0.47\\t 0.01\\t 0\\t0\\r\\nPctIlleg\\t\\t0\\t1\\t 0.25\\t 0.23\\t 0.74\\t 0.17\\t 0.09\\t0\\r\\nNumImmig\\t\\t0\\t1\\t 0.03\\t 0.09\\t 0.29\\t 0.01\\t 0\\t0\\r\\nPctImmigRecent\\t\\t0\\t1\\t 0.32\\t 0.22\\t 0.17\\t 0.29\\t 0\\t0\\r\\nPctImmigRec5\\t\\t0\\t1\\t 0.36\\t 0.21\\t 0.22\\t 0.34\\t 0\\t0\\r\\nPctImmigRec8\\t\\t0\\t1\\t 0.40\\t 0.20\\t 0.25\\t 0.39\\t 0.26\\t0\\r\\nPctImmigRec10\\t\\t0\\t1\\t 0.43\\t 0.19\\t 0.29\\t 0.43\\t 0.43\\t0\\r\\nPctRecentImmig\\t\\t0\\t1\\t 0.18\\t 0.24\\t 0.23\\t 0.09\\t 0.01\\t0\\r\\nPctRecImmig5\\t\\t0\\t1\\t 0.18\\t 0.24\\t 0.25\\t 0.08\\t 0.02\\t0\\r\\nPctRecImmig8\\t\\t0\\t1\\t 0.18\\t 0.24\\t 0.25\\t 0.09\\t 0.02\\t0\\r\\nPctRecImmig10\\t\\t0\\t1\\t 0.18\\t 0.23\\t 0.26\\t 0.09\\t 0.02\\t0\\r\\nPctSpeakEnglOnly\\t0\\t1\\t 0.79\\t 0.23\\t-0.24\\t 0.87\\t 0.96\\t0\\r\\nPctNotSpeakEnglWell\\t0\\t1\\t 0.15\\t 0.22\\t 0.30\\t 0.06\\t 0.03\\t0\\r\\nPctLargHouseFam\\t\\t0\\t1\\t 0.27\\t 0.20\\t 0.38\\t 0.2\\t 0.17\\t0\\r\\nPctLargHouseOccup\\t0\\t1\\t 0.25\\t 0.19\\t 0.29\\t 0.19\\t 0.19\\t0\\r\\nPersPerOccupHous\\t0\\t1\\t 0.46\\t 0.17\\t-0.04\\t 0.44\\t 0.37\\t0\\r\\nPersPerOwnOccHous\\t0\\t1\\t 0.49\\t 0.16\\t-0.12\\t 0.48\\t 0.45\\t0\\r\\nPersPerRentOccHous\\t0\\t1\\t 0.40\\t 0.19\\t 0.25\\t 0.36\\t 0.32\\t0\\r\\nPctPersOwnOccup\\t\\t0\\t1\\t 0.56\\t 0.20\\t-0.53\\t 0.56\\t 0.54\\t0\\r\\nPctPersDenseHous\\t0\\t1\\t 0.19\\t 0.21\\t 0.45\\t 0.11\\t 0.06\\t0\\r\\nPctHousLess3BR\\t\\t0\\t1\\t 0.50\\t 0.17\\t 0.47\\t 0.51\\t 0.53\\t0\\r\\nMedNumBR\\t\\t0\\t1\\t 0.31\\t 0.26\\t-0.36\\t 0.5\\t 0.5\\t0\\r\\nHousVacant\\t\\t0\\t1\\t 0.08\\t 0.15\\t 0.42\\t 0.03\\t 0.01\\t0\\r\\nPctHousOccup\\t\\t0\\t1\\t 0.72\\t 0.19\\t-0.32\\t 0.77\\t 0.88\\t0\\r\\nPctHousOwnOcc\\t\\t0\\t1\\t 0.55\\t 0.19\\t-0.47\\t 0.54\\t 0.52\\t0\\r\\nPctVacantBoarded\\t0\\t1\\t 0.20\\t 0.22\\t 0.48\\t 0.13\\t 0\\t0\\r\\nPctVacMore6Mos\\t\\t0\\t1\\t 0.43\\t 0.19\\t 0.02\\t 0.42\\t 0.44\\t0\\r\\nMedYrHousBuilt\\t\\t0\\t1\\t 0.49\\t 0.23\\t-0.11\\t 0.52\\t 0\\t0\\r\\nPctHousNoPhone\\t\\t0\\t1\\t 0.26\\t 0.24\\t 0.49\\t 0.185\\t 0.01\\t0\\r\\nPctWOFullPlumb\\t\\t0\\t1\\t 0.24\\t 0.21\\t 0.36\\t 0.19\\t 0\\t0\\r\\nOwnOccLowQuart\\t\\t0\\t1\\t 0.26\\t 0.22\\t-0.21\\t 0.18\\t 0.09\\t0\\r\\nOwnOccMedVal\\t\\t0\\t1\\t 0.26\\t 0.23\\t-0.19\\t 0.17\\t 0.08\\t0\\r\\nOwnOccHiQuart\\t\\t0\\t1\\t 0.27\\t 0.24\\t-0.17\\t 0.18\\t 0.08\\t0\\r\\nRentLowQ\\t\\t0\\t1\\t 0.35\\t 0.22\\t-0.25\\t 0.31\\t 0.13\\t0\\r\\nRentMedian\\t\\t0\\t1\\t 0.37\\t 0.21\\t-0.24\\t 0.33\\t 0.19\\t0\\r\\nRentHighQ\\t\\t0\\t1\\t 0.42\\t 0.25\\t-0.23\\t 0.37\\t 1\\t0\\r\\nMedRent\\t\\t\\t0\\t1\\t 0.38\\t 0.21\\t-0.24\\t 0.34\\t 0.17\\t0\\r\\nMedRentPctHousInc\\t0\\t1\\t 0.49\\t 0.17\\t 0.33\\t 0.48\\t 0.4\\t0\\r\\nMedOwnCostPctInc\\t0\\t1\\t 0.45\\t 0.19\\t 0.06\\t 0.45\\t 0.41\\t0\\r\\nMedOwnCostPctIncNoMtg\\t0\\t1\\t 0.40\\t 0.19\\t 0.05\\t 0.37\\t 0.24\\t0\\r\\nNumInShelters\\t\\t0\\t1\\t 0.03\\t 0.10\\t 0.38\\t 0\\t 0\\t0\\r\\nNumStreet\\t\\t0\\t1\\t 0.02\\t 0.10\\t 0.34\\t 0\\t 0\\t0\\r\\nPctForeignBorn\\t\\t0\\t1\\t 0.22\\t 0.23\\t 0.19\\t 0.13\\t 0.03\\t0\\r\\nPctBornSameState\\t0\\t1\\t 0.61\\t 0.20\\t-0.08\\t 0.63\\t 0.78\\t0\\r\\nPctSameHouse85\\t\\t0\\t1\\t 0.54\\t 0.18\\t-0.16\\t 0.54\\t 0.59\\t0\\r\\nPctSameCity85\\t\\t0\\t1\\t 0.63\\t 0.20\\t 0.08\\t 0.67\\t 0.74\\t0\\r\\nPctSameState85\\t\\t0\\t1\\t 0.65\\t 0.20\\t-0.02\\t 0.7\\t 0.79\\t0\\r\\nLemasSwornFT\\t\\t0\\t1\\t 0.07\\t 0.14\\t 0.34\\t 0.02\\t 0.02\\t1675\\r\\nLemasSwFTPerPop\\t\\t0\\t1\\t 0.22\\t 0.16\\t 0.15\\t 0.18\\t 0.2\\t1675\\r\\nLemasSwFTFieldOps\\t0\\t1\\t 0.92\\t 0.13\\t-0.33\\t 0.97\\t 0.98\\t1675\\r\\nLemasSwFTFieldPerPop\\t0\\t1\\t 0.25\\t 0.16\\t 0.16\\t 0.21\\t 0.19\\t1675\\r\\nLemasTotalReq\\t\\t0\\t1\\t 0.10\\t 0.16\\t 0.35\\t 0.04\\t 0.02\\t1675\\r\\nLemasTotReqPerPop\\t0\\t1\\t 0.22\\t 0.16\\t 0.27\\t 0.17\\t 0.14\\t1675\\r\\nPolicReqPerOffic\\t0\\t1\\t 0.34\\t 0.20\\t 0.17\\t 0.29\\t 0.23\\t1675\\r\\nPolicPerPop\\t\\t0\\t1\\t 0.22\\t 0.16\\t 0.15\\t 0.18\\t 0.2\\t1675\\r\\nRacialMatchCommPol\\t0\\t1\\t 0.69\\t 0.23\\t-0.46\\t 0.74\\t 0.78\\t1675\\r\\nPctPolicWhite\\t\\t0\\t1\\t 0.73\\t 0.22\\t-0.44\\t 0.78\\t 0.72\\t1675\\r\\nPctPolicBlack\\t\\t0\\t1\\t 0.22\\t 0.24\\t 0.54\\t 0.12\\t 0\\t1675\\r\\nPctPolicHisp\\t\\t0\\t1\\t 0.13\\t 0.20\\t 0.12\\t 0.06\\t 0\\t1675\\r\\nPctPolicAsian\\t\\t0\\t1\\t 0.11\\t 0.23\\t 0.10\\t 0\\t 0\\t1675\\r\\nPctPolicMinor\\t\\t0\\t1\\t 0.26\\t 0.23\\t 0.49\\t 0.2\\t 0.07\\t1675\\r\\nOfficAssgnDrugUnits\\t0\\t1\\t 0.08\\t 0.12\\t 0.34\\t 0.04\\t 0.03\\t1675\\r\\nNumKindsDrugsSeiz\\t0\\t1\\t 0.56\\t 0.20\\t 0.13\\t 0.57\\t 0.57\\t1675\\r\\nPolicAveOTWorked\\t0\\t1\\t 0.31\\t 0.23\\t 0.03\\t 0.26\\t 0.19\\t1675\\r\\nLandArea\\t\\t0\\t1\\t 0.07\\t 0.11\\t 0.20\\t 0.04\\t 0.01\\t0\\r\\nPopDens\\t\\t\\t0\\t1\\t 0.23\\t 0.20\\t 0.28\\t 0.17\\t 0.09\\t0\\r\\nPctUsePubTrans\\t\\t0\\t1\\t 0.16\\t 0.23\\t 0.15\\t 0.07\\t 0.01\\t0\\r\\nPolicCars\\t\\t0\\t1\\t 0.16\\t 0.21\\t 0.38\\t 0.08\\t 0.02\\t1675\\r\\nPolicOperBudg\\t\\t0\\t1\\t 0.08\\t 0.14\\t 0.34\\t 0.03\\t 0.02\\t1675\\r\\nLemasPctPolicOnPatr\\t0\\t1\\t 0.70\\t 0.21\\t-0.08\\t 0.75\\t 0.74\\t1675\\r\\nLemasGangUnitDeploy\\t0\\t1\\t 0.44\\t 0.41\\t 0.12\\t 0.5\\t 0\\t1675\\r\\nLemasPctOfficDrugUn\\t0\\t1\\t 0.09\\t 0.24\\t 0.35\\t 0\\t 0\\t0\\r\\nPolicBudgPerPop\\t\\t0\\t1\\t 0.20\\t 0.16\\t 0.10\\t 0.15\\t 0.12\\t1675\\r\\nViolentCrimesPerPop\\t0\\t1\\t 0.24\\t 0.23\\t 1.00\\t 0.15\\t 0.03\\t0\\r\\n\\r\\nDistribution of the Goal Variable (Violent Crimes per Population):\\r\\n   Range\\tFrequency\\r\\n0.000-0.067\\t484\\r\\n0.067-0.133\\t420\\r\\n0.133-0.200\\t284\\r\\n0.200-0.267\\t177\\r\\n0.267-0.333\\t142\\r\\n0.333-0.400\\t113\\r\\n0.400-0.467\\t 59\\r\\n0.467-0.533\\t 76\\r\\n0.533-0.600\\t 57\\r\\n0.600-0.667\\t 38\\r\\n0.667-0.733\\t 37\\r\\n0.733-0.800\\t 20\\r\\n0.800-0.867\\t 23\\r\\n0.867-0.933\\t 14\\r\\n0.933-1.000\\t 50\\r\\n\", 'citation': None}}\n",
      "                    name     role         type demographic description units  \\\n",
      "0                  state  Feature      Integer        None        None  None   \n",
      "1                 county  Feature      Integer        None        None  None   \n",
      "2              community  Feature      Integer        None        None  None   \n",
      "3          communityname  Feature  Categorical        None        None  None   \n",
      "4                   fold  Feature      Integer        None        None  None   \n",
      "..                   ...      ...          ...         ...         ...   ...   \n",
      "123  LemasPctPolicOnPatr  Feature   Continuous        None        None  None   \n",
      "124  LemasGangUnitDeploy  Feature      Integer        None        None  None   \n",
      "125  LemasPctOfficDrugUn  Feature      Integer        None        None  None   \n",
      "126      PolicBudgPerPop  Feature      Integer        None        None  None   \n",
      "127  ViolentCrimesPerPop   Target   Continuous        None        None  None   \n",
      "\n",
      "    missing_values  \n",
      "0               no  \n",
      "1              yes  \n",
      "2              yes  \n",
      "3               no  \n",
      "4               no  \n",
      "..             ...  \n",
      "123            yes  \n",
      "124            yes  \n",
      "125             no  \n",
      "126            yes  \n",
      "127             no  \n",
      "\n",
      "[128 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "communities_and_crime = fetch_ucirepo(id=183) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = communities_and_crime.data.features \n",
    "y = communities_and_crime.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(communities_and_crime.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(communities_and_crime.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "      <th>community</th>\n",
       "      <th>communityname</th>\n",
       "      <th>fold</th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>...</th>\n",
       "      <th>PolicAveOTWorked</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>PolicCars</th>\n",
       "      <th>PolicOperBudg</th>\n",
       "      <th>LemasPctPolicOnPatr</th>\n",
       "      <th>LemasGangUnitDeploy</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "      <th>PolicBudgPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>Lakewoodcity</td>\n",
       "      <td>1</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>Tukwilacity</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.45</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.45</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>Aberdeentown</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>81440</td>\n",
       "      <td>Willingborotownship</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.28</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>95</td>\n",
       "      <td>6096</td>\n",
       "      <td>Bethlehemtownship</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   state county community        communityname  fold  population  \\\n",
       "0      8      ?         ?         Lakewoodcity     1        0.19   \n",
       "1     53      ?         ?          Tukwilacity     1        0.00   \n",
       "2     24      ?         ?         Aberdeentown     1        0.00   \n",
       "3     34      5     81440  Willingborotownship     1        0.04   \n",
       "4     42     95      6096    Bethlehemtownship     1        0.01   \n",
       "\n",
       "   householdsize  racepctblack  racePctWhite  racePctAsian  ...  \\\n",
       "0           0.33          0.02          0.90          0.12  ...   \n",
       "1           0.16          0.12          0.74          0.45  ...   \n",
       "2           0.42          0.49          0.56          0.17  ...   \n",
       "3           0.77          1.00          0.08          0.12  ...   \n",
       "4           0.55          0.02          0.95          0.09  ...   \n",
       "\n",
       "   PolicAveOTWorked  LandArea  PopDens  PctUsePubTrans  PolicCars  \\\n",
       "0              0.29      0.12     0.26            0.20       0.06   \n",
       "1                 ?      0.02     0.12            0.45          ?   \n",
       "2                 ?      0.01     0.21            0.02          ?   \n",
       "3                 ?      0.02     0.39            0.28          ?   \n",
       "4                 ?      0.04     0.09            0.02          ?   \n",
       "\n",
       "   PolicOperBudg  LemasPctPolicOnPatr  LemasGangUnitDeploy  \\\n",
       "0           0.04                  0.9                  0.5   \n",
       "1              ?                    ?                    ?   \n",
       "2              ?                    ?                    ?   \n",
       "3              ?                    ?                    ?   \n",
       "4              ?                    ?                    ?   \n",
       "\n",
       "   LemasPctOfficDrugUn  PolicBudgPerPop  \n",
       "0                 0.32             0.14  \n",
       "1                 0.00                ?  \n",
       "2                 0.00                ?  \n",
       "3                 0.00                ?  \n",
       "4                 0.00                ?  \n",
       "\n",
       "[5 rows x 127 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>agePct12t29</th>\n",
       "      <th>agePct16t24</th>\n",
       "      <th>agePct65up</th>\n",
       "      <th>...</th>\n",
       "      <th>PolicAveOTWorked</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>PolicCars</th>\n",
       "      <th>PolicOperBudg</th>\n",
       "      <th>LemasPctPolicOnPatr</th>\n",
       "      <th>LemasGangUnitDeploy</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "      <th>PolicBudgPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.45</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.21</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.28</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.36</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   population  householdsize  racepctblack  racePctWhite  racePctAsian  \\\n",
       "0        0.19           0.33          0.02          0.90          0.12   \n",
       "1        0.00           0.16          0.12          0.74          0.45   \n",
       "2        0.00           0.42          0.49          0.56          0.17   \n",
       "3        0.04           0.77          1.00          0.08          0.12   \n",
       "4        0.01           0.55          0.02          0.95          0.09   \n",
       "\n",
       "   racePctHisp  agePct12t21  agePct12t29  agePct16t24  agePct65up  ...  \\\n",
       "0         0.17         0.34         0.47         0.29        0.32  ...   \n",
       "1         0.07         0.26         0.59         0.35        0.27  ...   \n",
       "2         0.04         0.39         0.47         0.28        0.32  ...   \n",
       "3         0.10         0.51         0.50         0.34        0.21  ...   \n",
       "4         0.05         0.38         0.38         0.23        0.36  ...   \n",
       "\n",
       "   PolicAveOTWorked  LandArea  PopDens  PctUsePubTrans  PolicCars  \\\n",
       "0              0.29      0.12     0.26            0.20       0.06   \n",
       "1                 ?      0.02     0.12            0.45          ?   \n",
       "2                 ?      0.01     0.21            0.02          ?   \n",
       "3                 ?      0.02     0.39            0.28          ?   \n",
       "4                 ?      0.04     0.09            0.02          ?   \n",
       "\n",
       "   PolicOperBudg  LemasPctPolicOnPatr  LemasGangUnitDeploy  \\\n",
       "0           0.04                  0.9                  0.5   \n",
       "1              ?                    ?                    ?   \n",
       "2              ?                    ?                    ?   \n",
       "3              ?                    ?                    ?   \n",
       "4              ?                    ?                    ?   \n",
       "\n",
       "   LemasPctOfficDrugUn  PolicBudgPerPop  \n",
       "0                 0.32             0.14  \n",
       "1                 0.00                ?  \n",
       "2                 0.00                ?  \n",
       "3                 0.00                ?  \n",
       "4                 0.00                ?  \n",
       "\n",
       "[5 rows x 122 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.drop(['state', 'county', 'community', 'communityname', 'fold'], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ViolentCrimesPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ViolentCrimesPerPop\n",
       "0                 0.20\n",
       "1                 0.67\n",
       "2                 0.43\n",
       "3                 0.12\n",
       "4                 0.03"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OtherPerCap                1\n",
      "LemasSwornFT            1675\n",
      "LemasSwFTPerPop         1675\n",
      "LemasSwFTFieldOps       1675\n",
      "LemasSwFTFieldPerPop    1675\n",
      "LemasTotalReq           1675\n",
      "LemasTotReqPerPop       1675\n",
      "PolicReqPerOffic        1675\n",
      "PolicPerPop             1675\n",
      "RacialMatchCommPol      1675\n",
      "PctPolicWhite           1675\n",
      "PctPolicBlack           1675\n",
      "PctPolicHisp            1675\n",
      "PctPolicAsian           1675\n",
      "PctPolicMinor           1675\n",
      "OfficAssgnDrugUnits     1675\n",
      "NumKindsDrugsSeiz       1675\n",
      "PolicAveOTWorked        1675\n",
      "PolicCars               1675\n",
      "PolicOperBudg           1675\n",
      "LemasPctPolicOnPatr     1675\n",
      "LemasGangUnitDeploy     1675\n",
      "PolicBudgPerPop         1675\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X.replace('?', np.nan, inplace=True)\n",
    "\n",
    "null_counts = X.isnull().sum()\n",
    "\n",
    "# Filter columns with more than 0 nulls\n",
    "null_counts = null_counts[null_counts > 0]\n",
    "\n",
    "# Print the result\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns where conversion to numeric fails\n",
    "for col in X.columns:\n",
    "    try:\n",
    "        X[col] = pd.to_numeric(X[col], errors='raise')\n",
    "    except ValueError:\n",
    "        print(f\"Column '{col}' contains non-numeric values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes[X.dtypes != np.float64]\n",
    "# Good, everything has been converted to numerical !\n",
    "# Now we can deal with Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of       population  householdsize  racepctblack  racePctWhite  racePctAsian  \\\n",
       "0           0.19           0.33          0.02          0.90          0.12   \n",
       "1           0.00           0.16          0.12          0.74          0.45   \n",
       "2           0.00           0.42          0.49          0.56          0.17   \n",
       "3           0.04           0.77          1.00          0.08          0.12   \n",
       "4           0.01           0.55          0.02          0.95          0.09   \n",
       "...          ...            ...           ...           ...           ...   \n",
       "1989        0.01           0.40          0.10          0.87          0.12   \n",
       "1990        0.05           0.96          0.46          0.28          0.83   \n",
       "1991        0.16           0.37          0.25          0.69          0.04   \n",
       "1992        0.08           0.51          0.06          0.87          0.22   \n",
       "1993        0.20           0.78          0.14          0.46          0.24   \n",
       "\n",
       "      racePctHisp  agePct12t21  agePct12t29  agePct16t24  agePct65up  ...  \\\n",
       "0            0.17         0.34         0.47         0.29        0.32  ...   \n",
       "1            0.07         0.26         0.59         0.35        0.27  ...   \n",
       "2            0.04         0.39         0.47         0.28        0.32  ...   \n",
       "3            0.10         0.51         0.50         0.34        0.21  ...   \n",
       "4            0.05         0.38         0.38         0.23        0.36  ...   \n",
       "...           ...          ...          ...          ...         ...  ...   \n",
       "1989         0.16         0.43         0.51         0.35        0.30  ...   \n",
       "1990         0.32         0.69         0.86         0.73        0.14  ...   \n",
       "1991         0.25         0.35         0.50         0.31        0.54  ...   \n",
       "1992         0.10         0.58         0.74         0.63        0.41  ...   \n",
       "1993         0.77         0.50         0.62         0.40        0.17  ...   \n",
       "\n",
       "      NumStreet  PctForeignBorn  PctBornSameState  PctSameHouse85  \\\n",
       "0          0.00            0.12              0.42            0.50   \n",
       "1          0.00            0.21              0.50            0.34   \n",
       "2          0.00            0.14              0.49            0.54   \n",
       "3          0.00            0.19              0.30            0.73   \n",
       "4          0.00            0.11              0.72            0.64   \n",
       "...         ...             ...               ...             ...   \n",
       "1989       0.00            0.22              0.28            0.34   \n",
       "1990       0.00            0.53              0.25            0.17   \n",
       "1991       0.02            0.25              0.68            0.61   \n",
       "1992       0.01            0.45              0.64            0.54   \n",
       "1993       0.08            0.68              0.50            0.34   \n",
       "\n",
       "      PctSameCity85  PctSameState85  LandArea  PopDens  PctUsePubTrans  \\\n",
       "0              0.51            0.64      0.12     0.26            0.20   \n",
       "1              0.60            0.52      0.02     0.12            0.45   \n",
       "2              0.67            0.56      0.01     0.21            0.02   \n",
       "3              0.64            0.65      0.02     0.39            0.28   \n",
       "4              0.61            0.53      0.04     0.09            0.02   \n",
       "...             ...             ...       ...      ...             ...   \n",
       "1989           0.48            0.39      0.01     0.28            0.05   \n",
       "1990           0.10            0.00      0.02     0.37            0.20   \n",
       "1991           0.79            0.76      0.08     0.32            0.18   \n",
       "1992           0.59            0.52      0.03     0.38            0.33   \n",
       "1993           0.35            0.68      0.11     0.30            0.05   \n",
       "\n",
       "      LemasPctOfficDrugUn  \n",
       "0                    0.32  \n",
       "1                    0.00  \n",
       "2                    0.00  \n",
       "3                    0.00  \n",
       "4                    0.00  \n",
       "...                   ...  \n",
       "1989                 0.00  \n",
       "1990                 0.00  \n",
       "1991                 0.91  \n",
       "1992                 0.22  \n",
       "1993                 1.00  \n",
       "\n",
       "[1994 rows x 100 columns]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['OtherPerCap'] = X['OtherPerCap'].fillna(X['OtherPerCap'].mean()) # Since there is 1 value missing, we replace with the mean\n",
    "\n",
    "null_counts.drop('OtherPerCap', inplace=True) # We drop the 'OtherPerCap' from null_count table since this column has no missing values anymore\n",
    "X.drop(null_counts.index.to_list(), axis=1, inplace=True) # For other, since there is too many missing, values, we drop this features from X\n",
    "X.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "Now that the preprocessing is done, having a X and y clean, let's implements the assigmenet methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if the method given to compute the score  exists: \n",
    "score_methods_array = ['RMSE', ]\n",
    "\n",
    "def method_error(score_used):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation and leave-one-out method\n",
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's code the function eval_MSE used in ISLP to evalutate the MSE given the features we want to include in the model\n",
    "def evalMSE_for_linear_regression(terms, X_train, y_train, X_test, y_test):\n",
    "    '''\n",
    "    Calculate the Mean Squared Error (MSE) between predicted and actual values.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    terms : array-like, shape (n_selected_features,)\n",
    "        The predictor variables (or features) used in the model.\n",
    "        \n",
    "    X_train : array-like, shape (n_samples_train, n_features)\n",
    "        Training data from the original preprocessed dataFrame (without the target column)\n",
    "        \n",
    "    y_test : array-like, shape (n_samples_train,)\n",
    "        The target value column of the original preprocessed dataFrame used for training.\n",
    "\n",
    "    X_test : array-like, shape (n_samples_test, n_features)\n",
    "        Testing data from the original preprocessed dataFrame (without the target column)\n",
    "        \n",
    "    y_test : array-like, shape (n_samples_test,)\n",
    "        The target value column of the original preprocessed dataFrame used for testing.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    mse : float\n",
    "        The calculated mean squared error on the test data.\n",
    "    '''\n",
    "\n",
    "    #intercept_train = pd.DataFrame({'intercept' : np.ones(X_train.shape[0])}, index=X_train.index)\n",
    "    #X_train = pd.concat([intercept_train, X_train[terms]], axis=1) # Construction of the X_train transform to use the linear regression of sklearn or statsmodels\n",
    "\n",
    "    #intercept_test = pd.DataFrame({'intercept' : np.ones(X_train.shape[0])}, index=X_train.index)\n",
    "    # X_test = pd.concat([intercept_test, X_test[terms]], axis=1)\n",
    "\n",
    "    # I wanted to do it by hand but the .predict function needs .fit_transform and .transform specifically.\n",
    "\n",
    "    mm = MS(terms)\n",
    "\n",
    "    X_train = mm.fit_transform(X_train)\n",
    "    X_test = mm.transform(X_test)\n",
    "\n",
    "    y_train = y_train['ViolentCrimesPerPop']\n",
    "    y_test = y_test['ViolentCrimesPerPop']\n",
    "\n",
    "    results = sm.OLS(y_train, X_train).fit()\n",
    "    test_pred = results.predict(X_test)\n",
    "\n",
    "    return np.mean((y_test - test_pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_CV(X, y, terms, k=5, tqdm_disable=\"False\"):\n",
    "    '''\n",
    "    Calculate the MSE using k-fold Cross Validation method.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Data from the original preprocessed dataFrame (without the target column)\n",
    "        \n",
    "    y : array-like, shape (n_samples,)\n",
    "        The target value column of the original preprocessed dataFrame.\n",
    "\n",
    "    terms : array-like, shape (n_selected_features,)\n",
    "        The predictor variables (or features) used in the model.\n",
    "        \n",
    "    k : integer\n",
    "        number of cross validation\n",
    "\n",
    "    tqdm_disable : Bool\n",
    "        Enable it when it is too heavy to watch the process of the loops, especially when this function is called numerous times\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    mse : float\n",
    "        The mean of different calculated mean squared error on the test data for different folds.\n",
    "    '''\n",
    "\n",
    "    # check if k is valid\n",
    "    if k > X.shape[0]:\n",
    "        raise ValueError(f\"k cannot be greater than the number of samples. k={k}, but X contains only {X.shape[0]} samples.\")\n",
    "\n",
    "    k_MSE = np.zeros(k)\n",
    "\n",
    "    X_shuffled, y_shuffled = shuffle(X, y, random_state=1)\n",
    "\n",
    "    X_parts = np.array_split(X_shuffled, k)\n",
    "    y_parts = np.array_split(y_shuffled, k)\n",
    "\n",
    "    for i in tqdm(range(k), disable=tqdm_disable):\n",
    "        X_test = X_parts[i]\n",
    "        y_test = y_parts[i]\n",
    "\n",
    "        # X_train = np.concatenate([part for j,part in enumerate(X_parts) if j != i])\n",
    "        # y_train = np.concatenate([part for j,part in enumerate(y_parts) if j != i]) \n",
    "        X_train = pd.concat([part for j,part in enumerate(X_parts) if j != i], axis=0)\n",
    "        y_train = pd.concat([part for j,part in enumerate(y_parts) if j != i], axis=0) \n",
    "        # [part for j,part in enumerate(X_parts) if j != i] generates an array of arrays of X_parts without the i-th part\n",
    "        # then it concatenates all the given arrays into one array X_train\n",
    "\n",
    "        k_MSE[i] = evalMSE_for_linear_regression(terms, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    return k_MSE.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\doria\\Documents\\pythonEnv\\dataScience\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 5/5 [00:00<00:00, 16.20it/s]\n"
     ]
    }
   ],
   "source": [
    "MSE_k5_Cross_Validation = k_fold_CV(X=X, y=y, terms=['population', 'householdsize', 'racepctblack', 'racePctWhite'], k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0252765560842217"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_k5_Cross_Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-one-out Cross Validation (LOOCV)\n",
    "I'm the function `k_fold_CV` with $k$ = to the number of datapoints of the preprocessed dataset `X` (lets say $N_X$). The function then divides `X` and `y` into $N_X$ folds. But in this case $N_X$ folds means that one fold is equal to one value of the dataset `X`,`y`. So during one iteration `i`of the `for` loop, it trains over the whole dataset except the `i`th value, and this for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_LOOCV = k_fold_CV(X=X, y=y, terms=['population', 'householdsize', 'racepctblack', 'racePctWhite'], k=X.shape[0]) \n",
    "# if k > length of the dataset, an error is raised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02535316426092495"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_LOOCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foward attribute selection\n",
    "\n",
    "To do:\n",
    "\n",
    "- Create a loop on `terms` to compute MSE using `k_fold_CV`\n",
    "- On each turn, saving the term/combination of terms that gives the best MSE.\n",
    "- then create a while loop (which includes the previous loop) that prevents overfitting by checking the MSE\n",
    "- extend the function to other score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foward_attribute_selection(X, y, k=5, score_used='MSE'):\n",
    "    '''\n",
    "    Returns the selected attributes that works best for training the model without overfitting. \n",
    "    Usage of foward attribute selection method.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Data from the original preprocessed dataFrame (without the target column)\n",
    "        \n",
    "    y : array-like, shape (n_samples,)\n",
    "        The target value column of the original preprocessed dataFrame.\n",
    "        \n",
    "    k : integer\n",
    "        number of cross validation\n",
    "    \n",
    "    score_used : char\n",
    "        usage of score (R2, MSE, etc)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    selected_terms : array-like, variable shape\n",
    "        Selected attributes that works best for training the model without overfitting.\n",
    "    '''\n",
    "\n",
    "    all_terms = X.columns.values\n",
    "\n",
    "    \n",
    "    \n",
    "    if score_used == 'MSE':\n",
    "        lim_score = np.inf\n",
    "        compare = lambda a, b: a < b\n",
    "    elif score_used == 'R2':\n",
    "        lim_score = -np.inf\n",
    "        compare = lambda a, b: a > b\n",
    "    else:\n",
    "        raise ValueError(f\"Score {score_used} is not supported or does not exists. Choose your scores between `MSE` or `R2`.\")\n",
    "        \n",
    "    best_global_score = lim_score\n",
    "    best_current_score = lim_score\n",
    "\n",
    "    best_current_terms = np.array([])\n",
    "    best_global_terms = np.array([]) \n",
    "    # To implement backward aswell, do a if function as I did for the score_used and fill or not these two\n",
    "\n",
    "    while True:\n",
    "        best_current_score = lim_score\n",
    "\n",
    "        for term in all_terms:\n",
    "            testing_terms = np.hstack([best_global_terms, [term]])\n",
    "            score = k_fold_CV(X=X, y=y, terms=testing_terms, tqdm_disable=True)\n",
    "\n",
    "            if compare(score, best_current_score):\n",
    "                best_current_terms = testing_terms\n",
    "                best_current_score = score\n",
    "\n",
    "        print(f\"New term added: {best_current_terms[-1]} to {best_global_terms}.\")\n",
    "        print(f\"{score_used} for theses terms is: {best_current_score}.\")\n",
    "        if compare(best_current_score, best_global_score):\n",
    "            print(f\"It is a better score than {best_global_score}. New selected terms is {best_current_terms}\")\n",
    "            best_global_terms = best_current_terms\n",
    "            best_global_score = best_current_score\n",
    "        else: # maybe add a count, like 2 or 3 to to another loop to check if we are overfitting or if we may have another ehancement.\n",
    "            print(f\"OVERFITTING: Score does not imporve compare to the previous score {best_global_score}.\\n\")\n",
    "            print(f\"Final selected terms for training is {best_current_terms}\")\n",
    "            return(best_global_terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New term added: PctKids2Par to [].\n",
      "MSE for theses terms is: 0.024730824263345174.\n",
      "It is a better score than inf. New selected terms is ['PctKids2Par']\n",
      "New term added: racePctWhite to ['PctKids2Par'].\n",
      "MSE for theses terms is: 0.021820891012692772.\n",
      "It is a better score than 0.024730824263345174. New selected terms is ['PctKids2Par' 'racePctWhite']\n",
      "New term added: HousVacant to ['PctKids2Par' 'racePctWhite'].\n",
      "MSE for theses terms is: 0.02035807509749834.\n",
      "It is a better score than 0.021820891012692772. New selected terms is ['PctKids2Par' 'racePctWhite' 'HousVacant']\n",
      "New term added: pctUrban to ['PctKids2Par' 'racePctWhite' 'HousVacant'].\n",
      "MSE for theses terms is: 0.020027668209430183.\n",
      "It is a better score than 0.02035807509749834. New selected terms is ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban']\n",
      "New term added: PctWorkMom to ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban'].\n",
      "MSE for theses terms is: 0.019763471413352125.\n",
      "It is a better score than 0.020027668209430183. New selected terms is ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom']\n",
      "New term added: MalePctDivorce to ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom'].\n",
      "MSE for theses terms is: 0.019564860151763296.\n",
      "It is a better score than 0.019763471413352125. New selected terms is ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom'\n",
      " 'MalePctDivorce']\n",
      "New term added: PctIlleg to ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom'\n",
      " 'MalePctDivorce'].\n",
      "MSE for theses terms is: 0.01930872848355101.\n",
      "It is a better score than 0.019564860151763296. New selected terms is ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom'\n",
      " 'MalePctDivorce' 'PctIlleg']\n",
      "New term added: NumStreet to ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom'\n",
      " 'MalePctDivorce' 'PctIlleg'].\n",
      "MSE for theses terms is: 0.019152929819222644.\n",
      "It is a better score than 0.01930872848355101. New selected terms is ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom'\n",
      " 'MalePctDivorce' 'PctIlleg' 'NumStreet']\n",
      "New term added: numbUrban to ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom'\n",
      " 'MalePctDivorce' 'PctIlleg' 'NumStreet'].\n",
      "MSE for theses terms is: 0.01896612812182706.\n",
      "It is a better score than 0.019152929819222644. New selected terms is ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom'\n",
      " 'MalePctDivorce' 'PctIlleg' 'NumStreet' 'numbUrban']\n",
      "New term added: PctPersDenseHous to ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom'\n",
      " 'MalePctDivorce' 'PctIlleg' 'NumStreet' 'numbUrban'].\n",
      "MSE for theses terms is: 0.01885064981498961.\n",
      "It is a better score than 0.01896612812182706. New selected terms is ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom'\n",
      " 'MalePctDivorce' 'PctIlleg' 'NumStreet' 'numbUrban' 'PctPersDenseHous']\n",
      "New term added: racepctblack to ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom'\n",
      " 'MalePctDivorce' 'PctIlleg' 'NumStreet' 'numbUrban' 'PctPersDenseHous'].\n",
      "MSE for theses terms is: 0.018688329737225028.\n",
      "It is a better score than 0.01885064981498961. New selected terms is ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom'\n",
      " 'MalePctDivorce' 'PctIlleg' 'NumStreet' 'numbUrban' 'PctPersDenseHous'\n",
      " 'racepctblack']\n",
      "New term added: agePct12t29 to ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom'\n",
      " 'MalePctDivorce' 'PctIlleg' 'NumStreet' 'numbUrban' 'PctPersDenseHous'\n",
      " 'racepctblack'].\n",
      "MSE for theses terms is: 0.018595362426549772.\n",
      "It is a better score than 0.018688329737225028. New selected terms is ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom'\n",
      " 'MalePctDivorce' 'PctIlleg' 'NumStreet' 'numbUrban' 'PctPersDenseHous'\n",
      " 'racepctblack' 'agePct12t29']\n",
      "New term added: MedOwnCostPctIncNoMtg to ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom'\n",
      " 'MalePctDivorce' 'PctIlleg' 'NumStreet' 'numbUrban' 'PctPersDenseHous'\n",
      " 'racepctblack' 'agePct12t29'].\n",
      "MSE for theses terms is: 0.01851554649234508.\n",
      "It is a better score than 0.018595362426549772. New selected terms is ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom'\n",
      " 'MalePctDivorce' 'PctIlleg' 'NumStreet' 'numbUrban' 'PctPersDenseHous'\n",
      " 'racepctblack' 'agePct12t29' 'MedOwnCostPctIncNoMtg']\n",
      "New term added: OtherPerCap to ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom'\n",
      " 'MalePctDivorce' 'PctIlleg' 'NumStreet' 'numbUrban' 'PctPersDenseHous'\n",
      " 'racepctblack' 'agePct12t29' 'MedOwnCostPctIncNoMtg'].\n",
      "MSE for theses terms is: 0.0184599051936617.\n",
      "It is a better score than 0.01851554649234508. New selected terms is ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom'\n",
      " 'MalePctDivorce' 'PctIlleg' 'NumStreet' 'numbUrban' 'PctPersDenseHous'\n",
      " 'racepctblack' 'agePct12t29' 'MedOwnCostPctIncNoMtg' 'OtherPerCap']\n",
      "New term added: LemasPctOfficDrugUn to ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom'\n",
      " 'MalePctDivorce' 'PctIlleg' 'NumStreet' 'numbUrban' 'PctPersDenseHous'\n",
      " 'racepctblack' 'agePct12t29' 'MedOwnCostPctIncNoMtg' 'OtherPerCap'].\n",
      "MSE for theses terms is: 0.018434900440915485.\n",
      "It is a better score than 0.0184599051936617. New selected terms is ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom'\n",
      " 'MalePctDivorce' 'PctIlleg' 'NumStreet' 'numbUrban' 'PctPersDenseHous'\n",
      " 'racepctblack' 'agePct12t29' 'MedOwnCostPctIncNoMtg' 'OtherPerCap'\n",
      " 'LemasPctOfficDrugUn']\n",
      "New term added: TotalPctDiv to ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom'\n",
      " 'MalePctDivorce' 'PctIlleg' 'NumStreet' 'numbUrban' 'PctPersDenseHous'\n",
      " 'racepctblack' 'agePct12t29' 'MedOwnCostPctIncNoMtg' 'OtherPerCap'\n",
      " 'LemasPctOfficDrugUn'].\n",
      "MSE for theses terms is: 0.018414132441223634.\n",
      "It is a better score than 0.018434900440915485. New selected terms is ['PctKids2Par' 'racePctWhite' 'HousVacant' 'pctUrban' 'PctWorkMom'\n",
      " 'MalePctDivorce' 'PctIlleg' 'NumStreet' 'numbUrban' 'PctPersDenseHous'\n",
      " 'racepctblack' 'agePct12t29' 'MedOwnCostPctIncNoMtg' 'OtherPerCap'\n",
      " 'LemasPctOfficDrugUn' 'TotalPctDiv']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# disabling warnings for a clearer view\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mfoward_attribute_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[48], line 50\u001b[0m, in \u001b[0;36mfoward_attribute_selection\u001b[1;34m(X, y, k, score_used)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m all_terms:\n\u001b[0;32m     49\u001b[0m     testing_terms \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([best_global_terms, [term]])\n\u001b[1;32m---> 50\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mk_fold_CV\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtesting_terms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtqdm_disable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compare(score, best_current_score):\n\u001b[0;32m     53\u001b[0m         best_current_terms \u001b[38;5;241m=\u001b[39m testing_terms\n",
      "Cell \u001b[1;32mIn[47], line 50\u001b[0m, in \u001b[0;36mk_fold_CV\u001b[1;34m(X, y, terms, k, tqdm_disable)\u001b[0m\n\u001b[0;32m     46\u001b[0m     y_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([part \u001b[38;5;28;01mfor\u001b[39;00m j,part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(y_parts) \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m!=\u001b[39m i], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# [part for j,part in enumerate(X_parts) if j != i] generates an array of arrays of X_parts without the i-th part\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# then it concatenates all the given arrays into one array X_train\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     k_MSE[i] \u001b[38;5;241m=\u001b[39m \u001b[43mevalMSE_for_linear_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mterms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m k_MSE\u001b[38;5;241m.\u001b[39mmean()\n",
      "Cell \u001b[1;32mIn[11], line 36\u001b[0m, in \u001b[0;36mevalMSE_for_linear_regression\u001b[1;34m(terms, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#intercept_train = pd.DataFrame({'intercept' : np.ones(X_train.shape[0])}, index=X_train.index)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#X_train = pd.concat([intercept_train, X_train[terms]], axis=1) # Construction of the X_train transform to use the linear regression of sklearn or statsmodels\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# I wanted to do it by hand but the .predict function needs .fit_transform and .transform specifically.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m mm \u001b[38;5;241m=\u001b[39m MS(terms)\n\u001b[1;32m---> 36\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m X_test \u001b[38;5;241m=\u001b[39m mm\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m     39\u001b[0m y_train \u001b[38;5;241m=\u001b[39m y_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mViolentCrimesPerPop\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\doria\\Documents\\pythonEnv\\dataScience\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\doria\\Documents\\pythonEnv\\dataScience\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\doria\\Documents\\pythonEnv\\dataScience\\Lib\\site-packages\\ISLP\\models\\model_spec.py:268\u001b[0m, in \u001b[0;36mModelSpec.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_ \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoders_ \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_info_ \u001b[38;5;241m=\u001b[39m \u001b[43m_get_column_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m                                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_categorical_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_ordinal_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mcategorical_encoders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_encoders_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;66;03m# include each column as a Feature\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;66;03m# so that their columns are built if needed\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns_:\n",
      "File \u001b[1;32mc:\\Users\\doria\\Documents\\pythonEnv\\dataScience\\Lib\\site-packages\\ISLP\\models\\columns.py:176\u001b[0m, in \u001b[0;36m_get_column_info\u001b[1;34m(X, columns, is_categorical, is_ordinal, categorical_encoders)\u001b[0m\n\u001b[0;32m    169\u001b[0m         column_info[col] \u001b[38;5;241m=\u001b[39m Column(col,\n\u001b[0;32m    170\u001b[0m                                   name,\n\u001b[0;32m    171\u001b[0m                                   is_categorical[i],\n\u001b[0;32m    172\u001b[0m                                   is_ordinal[i],\n\u001b[0;32m    173\u001b[0m                                   \u001b[38;5;28mtuple\u001b[39m(columns),\n\u001b[0;32m    174\u001b[0m                                   encoder)\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m         Xcol \u001b[38;5;241m=\u001b[39m \u001b[43m_get_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m    177\u001b[0m         column_info[col] \u001b[38;5;241m=\u001b[39m Column(col,\n\u001b[0;32m    178\u001b[0m                                   name,\n\u001b[0;32m    179\u001b[0m                                   columns\u001b[38;5;241m=\u001b[39m(name,))\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m column_info\n",
      "File \u001b[1;32mc:\\Users\\doria\\Documents\\pythonEnv\\dataScience\\Lib\\site-packages\\ISLP\\models\\columns.py:113\u001b[0m, in \u001b[0;36m_get_column\u001b[1;34m(idx, X, loc)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloc\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loc:\n\u001b[1;32m--> 113\u001b[0m         col \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:   \u001b[38;5;66;03m# use iloc instead\u001b[39;00m\n\u001b[0;32m    115\u001b[0m         col \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39miloc[:, [idx]]\n",
      "File \u001b[1;32mc:\\Users\\doria\\Documents\\pythonEnv\\dataScience\\Lib\\site-packages\\pandas\\core\\indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[1;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\doria\\Documents\\pythonEnv\\dataScience\\Lib\\site-packages\\pandas\\core\\indexing.py:1377\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take_opportunity(tup):\n\u001b[0;32m   1375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take(tup)\n\u001b[1;32m-> 1377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple_same_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\doria\\Documents\\pythonEnv\\dataScience\\Lib\\site-packages\\pandas\\core\\indexing.py:1020\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_tuple_same_dim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_null_slice(key):\n\u001b[0;32m   1018\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 1020\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mretval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# We should never have retval.ndim < self.ndim, as that should\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m#  be handled by the _getitem_lowerdim call above.\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m retval\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim\n",
      "File \u001b[1;32mc:\\Users\\doria\\Documents\\pythonEnv\\dataScience\\Lib\\site-packages\\pandas\\core\\indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[0;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[1;32mc:\\Users\\doria\\Documents\\pythonEnv\\dataScience\\Lib\\site-packages\\pandas\\core\\indexing.py:1361\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m   1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_listlike_indexer(key, axis)\n\u001b[1;32m-> 1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_with_indexers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m   1363\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\doria\\Documents\\pythonEnv\\dataScience\\Lib\\site-packages\\pandas\\core\\generic.py:5686\u001b[0m, in \u001b[0;36mNDFrame._reindex_with_indexers\u001b[1;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[0;32m   5683\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m ensure_platform_int(indexer)\n\u001b[0;32m   5685\u001b[0m \u001b[38;5;66;03m# TODO: speed up on homogeneous DataFrame objects (see _reindex_multi)\u001b[39;00m\n\u001b[1;32m-> 5686\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mnew_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5689\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_dups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5693\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5694\u001b[0m \u001b[38;5;66;03m# If we've made a copy once, no need to make another one\u001b[39;00m\n\u001b[0;32m   5695\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\doria\\Documents\\pythonEnv\\dataScience\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:680\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested axis not found in manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slice_take_blocks_ax0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    689\u001b[0m             indexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\doria\\Documents\\pythonEnv\\dataScience\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:843\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[1;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[0;32m    841\u001b[0m                     blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[0;32m    842\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 843\u001b[0m                 nb \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m                 blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n",
      "File \u001b[1;32mc:\\Users\\doria\\Documents\\pythonEnv\\dataScience\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\doria\\Documents\\pythonEnv\\dataScience\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\doria\\Documents\\pythonEnv\\dataScience\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:162\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    161\u001b[0m )\n\u001b[1;32m--> 162\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[0;32m    165\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\") # disabling warnings for a clearer view\n",
    "foward_attribute_selection(X=X,y=y)\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
