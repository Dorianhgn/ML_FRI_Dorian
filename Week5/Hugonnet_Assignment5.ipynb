{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from matplotlib.pyplot import subplots \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting method\n",
    "So here, if I am right, there is two main methods : One for regression and one other for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pseudo-Code: Gradient Boosting for Regression**\n",
    "\n",
    "Here’s a structured and refined pseudo-code for Gradient Boosting for Regression based on your notes:\n",
    "\n",
    "---\n",
    "\n",
    "#### **Inputs**:\n",
    "- Training data: $ \\{(X_i, y_i)\\}_{i=1}^n $\n",
    "- Differentiable loss function: $ L(y_i, F(X)) = \\frac{1}{2} (y_i - F(X_i))^2 $\n",
    "- Learning rate: $ \\alpha $\n",
    "- Number of trees: $ M $\n",
    "- Maximum tree depth: $ d $\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 1: Initialization**\n",
    "1. Compute the **initial prediction** $ F_0 $ as the average of the target values:\n",
    "   $$\n",
    "   F_0 = \\frac{1}{n} \\sum_{i=1}^n y_i\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Gradient Boosting Loop**\n",
    "For $ m = 1 $ to $ M $ (number of trees):\n",
    "1. **Compute residuals**:\n",
    "   $$\n",
    "   r_{m, i} = -\\frac{\\partial L(y_i, F_{m-1}(X_i))}{\\partial F_{m-1}(X_i)}\n",
    "   $$\n",
    "   For $ L(y_i, F(X)) = \\frac{1}{2}(y_i - F(X))^2 $, this simplifies to:\n",
    "   $$\n",
    "   r_{m, i} = y_i - F_{m-1}(X_i)\n",
    "   $$\n",
    "\n",
    "2. **Fit a regression tree**:\n",
    "   - Train a regression tree $ T_m(X) $ of maximum depth $ d $ on the residuals $ r_{m, i} $.\n",
    "   - The tree splits the data into $ J $ terminal regions (leaves) $ R_{m, j} $.\n",
    "\n",
    "3. **Compute leaf values**:\n",
    "   For each leaf $ R_{m, j} $, compute:\n",
    "   $$\n",
    "   \\gamma_{m, j} = \\text{average of residuals in } R_{m, j}\n",
    "   $$\n",
    "   $$\n",
    "   \\gamma_{m, j} = \\frac{\\sum_{X_i \\in R_{m,j}} r_{m, i}}{|R_{m, j}|}\n",
    "   $$\n",
    "\n",
    "4. **Update the prediction model**:\n",
    "   For each sample $ X_i $:\n",
    "   $$\n",
    "   F_m(X_i) = F_{m-1}(X_i) + \\alpha \\cdot \\gamma_{m, j}, \\quad \\text{where } X_i \\in R_{m,j}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Final Prediction**\n",
    "For each sample $ X_i $, compute the final prediction:\n",
    "$$\n",
    "F_M(X_i) = F_0 + \\sum_{m=1}^M \\alpha \\cdot \\gamma_{m, j}, \\quad \\text{where } X_i \\in R_{m,j}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Notes**\n",
    "1. **Residuals**: Represent the negative gradient of the loss function with respect to the current predictions.\n",
    "2. **Leaf Values**: Each leaf value $ \\gamma_{m,j} $ corrects the predictions of the previous trees.\n",
    "3. **Learning Rate**: $ \\alpha $ controls the step size, preventing overfitting by scaling updates.\n",
    "4. **Stopping Criteria**: Typically, the number of trees $ M $ or early stopping (validation loss) is used.\n",
    "\n",
    "---\n",
    "\n",
    "This pseudo-code is precise and matches your notes while being easy to follow. Let me know if you’d like additional examples or diagrams to illustrate the steps!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pseudo-Code: Gradient Boosting for Classification**\n",
    "\n",
    "Here’s a structured and refined pseudo-code for Gradient Boosting for Classification based on your notes:\n",
    "\n",
    "---\n",
    "\n",
    "#### **Inputs**:\n",
    "- Training data: $ \\{(X_i, y_i)\\}_{i=1}^n $, where $ y_i \\in \\{0, 1\\} $ (binary classification)\n",
    "- Loss function: Negative log-likelihood:\n",
    "  $$\n",
    "  \\mathcal J  = -y_i \\ln(p) + \\ln(1 + e^{F(X_i)})\n",
    "  $$\n",
    "- Learning rate: $ \\alpha $\n",
    "- Number of trees: $ M $\n",
    "- Maximum tree depth: $ d $\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 1: Initialization**\n",
    "1. Compute the **initial log-odds**:\n",
    "   $$\n",
    "   F_0(X_i) = \\ln\\left(\\frac{\\text{\\# positive samples}}{\\text{\\# negative samples}}\\right)\n",
    "   $$\n",
    "2. Compute the **initial probability** for each sample:\n",
    "   $$\n",
    "   p_0(X_i) = \\sigma(F_0(X_i)) = \\frac{1}{1 + e^{-F_0(X_i)}}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Gradient Boosting Loop**\n",
    "For $ m = 1 $ to $ M $ (number of trees):\n",
    "1. **Compute pseudo-residuals**:\n",
    "   - Residuals are the negative gradient of the loss with respect to $ F(X_i) $:\n",
    "     $$\n",
    "     r_{m, i} = -\\frac{\\partial J}{\\partial F(X_i)} = y_i - p_{m-1}(X_i)\n",
    "     $$\n",
    "\n",
    "2. **Fit a regression tree**:\n",
    "   - Train a regression tree $ T_m(X) $ of maximum depth $ d $ on the pseudo-residuals $ r_{m, i} $.\n",
    "   - The tree splits the data into $ J $ terminal regions (leaves) $ R_{m,j} $.\n",
    "\n",
    "3. **Compute leaf values**:\n",
    "   - For each leaf $ R_{m, j} $, compute:\n",
    "     $$\n",
    "     \\gamma_{m,j} = \\frac{\\sum_{X_i \\in R_{m,j}} r_{m, i}}{\\sum_{X_i \\in R_{m,j}} p_{m-1}(X_i)(1 - p_{m-1}(X_i))}\n",
    "     $$\n",
    "     Where:\n",
    "     - $ p_{m-1}(X_i) = \\sigma(F_{m-1}(X_i)) = \\frac{1}{1 + e^{-F_{m-1}(X_i)}} $\n",
    "\n",
    "4. **Update the prediction model**:\n",
    "   - For each sample $ X_i $:\n",
    "     $$\n",
    "     F_m(X_i) = F_{m-1}(X_i) + \\alpha \\cdot \\gamma_{m,j}, \\quad \\text{where } X_i \\in R_{m,j}\n",
    "     $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Final Prediction**\n",
    "1. Compute the final prediction for each sample $ X_i $:\n",
    "   $$\n",
    "   \\hat{p}(X_i) = \\sigma(F_M(X_i)) = \\frac{1}{1 + e^{-F_M(X_i)}}\n",
    "   $$\n",
    "   Where:\n",
    "   $$\n",
    "   F_M(X_i) = F_0(X_i) + \\sum_{m=1}^M \\alpha \\cdot \\gamma_{m,j}, \\quad \\text{where } X_i \\in R_{m,j}\n",
    "   $$\n",
    "\n",
    "2. Classify samples based on the probability threshold (e.g., $ \\hat{p}(X_i) > 0.5 $).\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Notes**\n",
    "1. **Residuals**:\n",
    "   - Residuals $ r_{m,i} $ approximate the gradient of the loss function.\n",
    "   - These residuals are used to fit regression trees in each iteration.\n",
    "\n",
    "2. **Leaf Values ($ \\gamma_{m,j} $)**:\n",
    "   - Computed as the ratio of the sum of residuals to the sum of the product $ p(1-p) $ within each leaf.\n",
    "\n",
    "3. **Learning Rate ($ \\alpha $)**:\n",
    "   - Scales updates to prevent overfitting and stabilize learning.\n",
    "\n",
    "4. **Final Prediction**:\n",
    "   - Probabilities are derived from the final log-odds $ F_M(X_i) $ using the sigmoid function.\n",
    "\n",
    "---\n",
    "\n",
    "This refined pseudo-code covers all essential steps and aligns with your notes. Let me know if you'd like examples or additional clarifications!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed.acidity</th>\n",
       "      <th>volatile.acidity</th>\n",
       "      <th>citric.acid</th>\n",
       "      <th>residual.sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free.sulfur.dioxide</th>\n",
       "      <th>total.sulfur.dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed.acidity  volatile.acidity  citric.acid  residual.sugar  chlorides  \\\n",
       "1            7.0              0.27         0.36            20.7      0.045   \n",
       "2            6.3              0.30         0.34             1.6      0.049   \n",
       "3            8.1              0.28         0.40             6.9      0.050   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "5            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free.sulfur.dioxide  total.sulfur.dioxide  density    pH  sulphates  \\\n",
       "1                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "2                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "3                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "5                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "  alcohol_level  \n",
       "1           low  \n",
       "2           low  \n",
       "3          high  \n",
       "4           low  \n",
       "5           low  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/wine_quality.csv', index_col=0)\n",
    "\n",
    "X = df.drop(['quality', 'X'], axis=1)\n",
    "y = df.quality\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    low\n",
       "1  True\n",
       "2  True\n",
       "3  True\n",
       "4  True\n",
       "5  True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.get_dummies(X, drop_first=True)\n",
    "y = pd.get_dummies(y, drop_first=True)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed.acidity           0\n",
       "volatile.acidity        0\n",
       "citric.acid             0\n",
       "residual.sugar          0\n",
       "chlorides               0\n",
       "free.sulfur.dioxide     0\n",
       "total.sulfur.dioxide    0\n",
       "density                 0\n",
       "pH                      0\n",
       "sulphates               0\n",
       "alcohol_level_low       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deal with missing values\n",
    "X.isnull().sum()\n",
    "# Well no need to deal with them fortunately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the gradient boosting tree class\n",
    "class GradientBoostingTree:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.001, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.gammas = []\n",
    "        self.F_0 = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Step 1: Initialization\n",
    "        # 1.1 initial log-odds:\n",
    "        self.F_0 = np.log(np.mean(y) / (1 - np.mean(y)))\n",
    "        F_prev = np.ones(len(y)) * self.F_0\n",
    "\n",
    "        # Step 2: gradient boosting loop for m = 1 to M (n_estimators):\n",
    "        for m in tqdm(range(self.n_estimators)):\n",
    "            p_prev = 1 / (1 + np.exp(-F_prev))\n",
    "            # 2.1: Compute the pseudo-residuals\n",
    "            residuals = y - p_prev\n",
    "\n",
    "            # 2.2: Fit a regression tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Save the tree\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            # 2.3: Compute the gammas for each leaf (extract from tree)\n",
    "            leaf_indices = tree.apply(X)\n",
    "            \n",
    "            gamma = {}\n",
    "            for leaf in np.unique(leaf_indices):\n",
    "                # Get indices of samples in the current leaf. ndarray of shape (n_samples,)\n",
    "                in_leaf = leaf_indices == leaf\n",
    "\n",
    "                # the numerator is the sum of residuals in the leaf\n",
    "                numerator = np.sum(residuals[in_leaf])\n",
    "                # the denominator is the sum of the p(1-p) in the leaf\n",
    "                denominator = np.sum(p_prev[in_leaf] * (1 - p_prev[in_leaf]))\n",
    "\n",
    "                # Avoid division by zero\n",
    "                gamma[leaf] = numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "            # store the gamma values\n",
    "            self.gammas.append(gamma)\n",
    "\n",
    "            # 2.4: Uptade the prediction model \n",
    "            for i in range(len(F_prev)):\n",
    "                F_prev += self.learning_rate * gamma[leaf_indices[i]]\n",
    "\n",
    "    def predict(self, X):\n",
    "        F = np.ones(X.shape[0]) * self.F_0\n",
    "\n",
    "        for m in tqdm(range(self.n_estimators)):\n",
    "            gamma = self.gammas[m]\n",
    "            tree = self.trees[m]\n",
    "            leaf_indices = tree.apply(X)\n",
    "            for i in range(len(F)):\n",
    "                F[i] += self.learning_rate * gamma[leaf_indices[i]]\n",
    "\n",
    "        # Return the probability\n",
    "        return 1 / (1 + np.exp(-F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:05<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2, random_state=1)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "gbt = GradientBoostingTree(n_estimators=5, learning_rate=0.001, max_depth=3)\n",
    "gbt.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 1058.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss: 8.13\n",
      "Accuracy: 0.77\n",
      "F1 Score: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gbt.predict(X_test)\n",
    "y_pred_round = np.round(y_pred)\n",
    "\n",
    "print(f'log loss: {log_loss(y_test, y_pred):.2f}')\n",
    "# Evaluate the model with the accuracy and F1 score\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred_round):.2f}')\n",
    "print(f'F1 Score: {f1_score(y_test, y_pred_round):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(model, X, y, k=5, method='predict', n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Perform cross-validation with optional parallelization.\n",
    "    \n",
    "    Parameters:\n",
    "        model: The model to be cross-validated.\n",
    "        X: Features (numpy array or pandas DataFrame).\n",
    "        y: Target (numpy array or pandas Series).\n",
    "        k: Number of folds.\n",
    "        method: 'predict' or 'predict_proba'.\n",
    "        n_jobs: Number of parallel jobs (-1 to use all CPUs).\n",
    "    \n",
    "    Returns:\n",
    "        Mean log-loss score across folds.\n",
    "    \"\"\"\n",
    "    fold_size = len(X) // k\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    def process_fold(i):\n",
    "        # Split indices for the current fold\n",
    "        val_indices = indices[i * fold_size: (i + 1) * fold_size]\n",
    "        train_indices = np.concatenate([indices[:i * fold_size], indices[(i + 1) * fold_size:]])\n",
    "        \n",
    "        # Prepare train and validation sets\n",
    "        X_train, X_val = X[train_indices], X[val_indices]\n",
    "        y_train, y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "        # Scale the data\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        if method == 'predict':\n",
    "            y_pred = model.predict(X_val)\n",
    "        elif method == 'predict_proba':\n",
    "            y_pred = model.predict_proba(X_val)\n",
    "        else:\n",
    "            raise ValueError('Invalid method')\n",
    "\n",
    "        # Calculate log-loss for the current fold\n",
    "        return log_loss(y_val, y_pred)\n",
    "\n",
    "    # Parallelize the processing of each fold\n",
    "    log_losses_scores = Parallel(n_jobs=n_jobs)(delayed(process_fold)(i) for i in range(k))\n",
    "    \n",
    "    return np.mean(log_losses_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_validate(model, X, y, k=5, method='predict'):\n",
    "#     fold_size = len(X) // k\n",
    "#     indices = np.arange(len(X))\n",
    "#     np.random.shuffle(indices)\n",
    "    \n",
    "#     log_losses_scores = []\n",
    "    \n",
    "#     for i in range(k):\n",
    "#         val_indices = indices[i * fold_size: (i + 1) * fold_size]\n",
    "#         train_indices = np.concatenate([indices[:i * fold_size], indices[(i + 1) * fold_size:]])\n",
    "        \n",
    "#         X_train, X_val = X[train_indices], X[val_indices]\n",
    "#         y_train, y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "#         # scale the data\n",
    "#         scaler = StandardScaler()\n",
    "#         X_train = scaler.fit_transform(X_train)\n",
    "#         X_val = scaler.transform(X_val)\n",
    "        \n",
    "#         model.fit(X_train, y_train)\n",
    "#         if method == 'predict':\n",
    "#             y_pred = model.predict(X_val)\n",
    "#         elif method == 'predict_proba':\n",
    "#             y_pred = model.predict_proba(X_val)\n",
    "#         else:\n",
    "#             raise ValueError('Invalid method')\n",
    "\n",
    "#         log_loss_score = log_loss(y_val, y_pred)\n",
    "#         log_losses_scores.append(log_loss_score)\n",
    "\n",
    "#     return np.mean(log_losses_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform cross validation\n",
    "gbt = GradientBoostingTree(n_estimators=5, learning_rate=0.001, max_depth=3)\n",
    "log_loss_score = cross_validate(gbt, X.values, y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 5-fold Cross Validation log-loss score: 0.76\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean 5-fold Cross Validation log-loss score: {log_loss_score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test different learning rates\n",
    "Here for each learning rate, we are performing cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different learning rates\n",
    "learning_rates = np.logspace(-6, -3.5, 10)\n",
    "log_losses = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    gbt = GradientBoostingTree(n_estimators=3, learning_rate=lr, max_depth=3)\n",
    "    log_loss_score = cross_validate(gbt, X.values, y.values, n_jobs=-1)\n",
    "    log_losses.append(log_loss_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the results\n",
    "fig, ax = subplots()\n",
    "ax.plot(learning_rates, log_losses, marker='o')\n",
    "ax.set_xlabel('Learning rate')\n",
    "ax.set_ylabel('Log loss')\n",
    "ax.set_xscale('log')\n",
    "ax.set_title('Log loss vs learning rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate: 8.80e-05\n"
     ]
    }
   ],
   "source": [
    "# extract the best learning rate\n",
    "best_lr = learning_rates[np.argmin(log_losses)]\n",
    "print(f'Best learning rate: {best_lr:.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test different number of trees build for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate: 7.94e-05\n",
      "Best number of estimators: 3\n",
      "Best log loss: 0.4464\n",
      "    n_estimators  learning_rate  log_loss\n",
      "0              1       0.000010  0.516033\n",
      "1              2       0.000010  0.509481\n",
      "2              3       0.000010  0.503185\n",
      "3              4       0.000010  0.496420\n",
      "4              5       0.000010  0.492154\n",
      "5              1       0.000020  0.509742\n",
      "6              2       0.000020  0.496488\n",
      "7              3       0.000020  0.486744\n",
      "8              4       0.000020  0.475074\n",
      "9              5       0.000020  0.471537\n",
      "10             1       0.000040  0.496912\n",
      "11             2       0.000040  0.476396\n",
      "12             3       0.000040  0.460138\n",
      "13             4       0.000040  0.449367\n",
      "14             5       0.000040  0.449185\n",
      "15             1       0.000079  0.477863\n",
      "16             2       0.000079  0.453307\n",
      "17             3       0.000079  0.446374\n",
      "18             4       0.000079  0.473929\n",
      "19             5       0.000079  0.498852\n",
      "20             1       0.000158  0.454812\n",
      "21             2       0.000158  0.464238\n",
      "22             3       0.000158  0.535632\n",
      "23             4       0.000158  0.617989\n",
      "24             5       0.000158  0.764872\n"
     ]
    }
   ],
   "source": [
    "# Perform grid search\n",
    "n_estimators = np.arange(1, 6, 1)\n",
    "learning_rates = np.logspace(-5, -3.8, 5)\n",
    "results = []  # To store hyperparameters and log losses\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for n in n_estimators:\n",
    "        gbt = GradientBoostingTree(n_estimators=n, learning_rate=lr, max_depth=3)\n",
    "        log_loss_score = cross_validate(gbt, X.values, y.values, n_jobs=-1)\n",
    "        results.append({'n_estimators': n, 'learning_rate': lr, 'log_loss': log_loss_score})\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Find the best hyperparameters\n",
    "best_row = results_df.loc[results_df['log_loss'].idxmin()]\n",
    "\n",
    "print(f\"Best learning rate: {best_row['learning_rate']:.2e}\")\n",
    "print(f\"Best number of estimators: {int(best_row['n_estimators'])}\")\n",
    "print(f\"Best log loss: {best_row['log_loss']:.4f}\")\n",
    "\n",
    "# Display the full results DataFrame\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr = best_row['learning_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the results with Sklearn Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom model log loss: 0.45\n",
      "Sklearn model log loss: 0.52\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Configure the model with the best learning rate\n",
    "gbt = GradientBoostingTree(n_estimators=3, learning_rate=best_lr, max_depth=3)\n",
    "gbt_sklearn = GradientBoostingClassifier(n_estimators=3, learning_rate=best_lr, max_depth=3)\n",
    "\n",
    "# Compute the log loss for both models using 5-fold cross validation\n",
    "log_loss_score = cross_validate(gbt, X.values, y.values)\n",
    "sklearn_log_loss_score = cross_validate(gbt_sklearn, X.values, y.values, method='predict_proba')\n",
    "\n",
    "# Evaluate the model with the log loss for both models\n",
    "print(f'Custom model log loss: {log_loss_score:.2f}')\n",
    "print(f'Sklearn model log loss: {sklearn_log_loss_score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Best log loss: -0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\doria\\Documents\\pythonEnv\\dataScience\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [23:31:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, scoring='neg_log_loss', cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f'Best parameters: {best_params}')\n",
    "print(f'Best log loss: {best_score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
