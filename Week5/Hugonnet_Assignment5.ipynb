{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from matplotlib.pyplot import subplots \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement gradient boosting from scratch:\n",
    "So here, if I am right, there is two main methods : One for regression and one other for classification\n",
    "\n",
    "### **Pseudo-Code: Gradient Boosting for Regression**\n",
    "\n",
    "Here’s a structured and refined pseudo-code for Gradient Boosting for Regression based on your notes:\n",
    "\n",
    "---\n",
    "\n",
    "#### **Inputs**:\n",
    "- Training data: $ \\{(X_i, y_i)\\}_{i=1}^n $\n",
    "- Differentiable loss function: $ L(y_i, F(X)) = \\frac{1}{2} (y_i - F(X_i))^2 $\n",
    "- Learning rate: $ \\alpha $\n",
    "- Number of trees: $ M $\n",
    "- Maximum tree depth: $ d $\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 1: Initialization**\n",
    "1. Compute the **initial prediction** $ F_0 $ as the average of the target values:\n",
    "   $$\n",
    "   F_0 = \\frac{1}{n} \\sum_{i=1}^n y_i\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Gradient Boosting Loop**\n",
    "For $ m = 1 $ to $ M $ (number of trees):\n",
    "1. **Compute residuals**:\n",
    "   $$\n",
    "   r_{m, i} = -\\frac{\\partial L(y_i, F_{m-1}(X_i))}{\\partial F_{m-1}(X_i)}\n",
    "   $$\n",
    "   For $ L(y_i, F(X)) = \\frac{1}{2}(y_i - F(X))^2 $, this simplifies to:\n",
    "   $$\n",
    "   r_{m, i} = y_i - F_{m-1}(X_i)\n",
    "   $$\n",
    "\n",
    "2. **Fit a regression tree**:\n",
    "   - Train a regression tree $ T_m(X) $ of maximum depth $ d $ on the residuals $ r_{m, i} $.\n",
    "   - The tree splits the data into $ J $ terminal regions (leaves) $ R_{m, j} $.\n",
    "\n",
    "3. **Compute leaf values**:\n",
    "   For each leaf $ R_{m, j} $, compute:\n",
    "   $$\n",
    "   \\gamma_{m, j} = \\text{average of residuals in } R_{m, j}\n",
    "   $$\n",
    "   $$\n",
    "   \\gamma_{m, j} = \\frac{\\sum_{X_i \\in R_{m,j}} r_{m, i}}{|R_{m, j}|}\n",
    "   $$\n",
    "\n",
    "4. **Update the prediction model**:\n",
    "   For each sample $ X_i $:\n",
    "   $$\n",
    "   F_m(X_i) = F_{m-1}(X_i) + \\alpha \\cdot \\gamma_{m, j}, \\quad \\text{where } X_i \\in R_{m,j}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Final Prediction**\n",
    "For each sample $ X_i $, compute the final prediction:\n",
    "$$\n",
    "F_M(X_i) = F_0 + \\sum_{m=1}^M \\alpha \\cdot \\gamma_{m, j}, \\quad \\text{where } X_i \\in R_{m,j}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Notes**\n",
    "1. **Residuals**: Represent the negative gradient of the loss function with respect to the current predictions.\n",
    "2. **Leaf Values**: Each leaf value $ \\gamma_{m,j} $ corrects the predictions of the previous trees.\n",
    "3. **Learning Rate**: $ \\alpha $ controls the step size, preventing overfitting by scaling updates.\n",
    "4. **Stopping Criteria**: Typically, the number of trees $ M $ or early stopping (validation loss) is used.\n",
    "\n",
    "---\n",
    "\n",
    "This pseudo-code is precise and matches your notes while being easy to follow. Let me know if you’d like additional examples or diagrams to illustrate the steps!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pseudo-Code: Gradient Boosting for Classification**\n",
    "\n",
    "Here’s a structured and refined pseudo-code for Gradient Boosting for Classification based on your notes:\n",
    "\n",
    "---\n",
    "\n",
    "#### **Inputs**:\n",
    "- Training data: $ \\{(X_i, y_i)\\}_{i=1}^n $, where $ y_i \\in \\{0, 1\\} $ (binary classification)\n",
    "- Loss function: Negative log-likelihood:\n",
    "  $$\n",
    "  \\mathcal J  = -y_i \\ln(p) + \\ln(1 + e^{F(X_i)})\n",
    "  $$\n",
    "- Learning rate: $ \\alpha $\n",
    "- Number of trees: $ M $\n",
    "- Maximum tree depth: $ d $\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 1: Initialization**\n",
    "1. Compute the **initial log-odds**:\n",
    "   $$\n",
    "   F_0(X_i) = \\ln\\left(\\frac{\\text{\\# positive samples}}{\\text{\\# negative samples}}\\right)\n",
    "   $$\n",
    "2. Compute the **initial probability** for each sample:\n",
    "   $$\n",
    "   p_0(X_i) = \\sigma(F_0(X_i)) = \\frac{1}{1 + e^{-F_0(X_i)}}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Gradient Boosting Loop**\n",
    "For $ m = 1 $ to $ M $ (number of trees):\n",
    "1. **Compute pseudo-residuals**:\n",
    "   - Residuals are the negative gradient of the loss with respect to $ F(X_i) $:\n",
    "     $$\n",
    "     r_{m, i} = -\\frac{\\partial J}{\\partial F(X_i)} = y_i - p_{m-1}(X_i)\n",
    "     $$\n",
    "\n",
    "2. **Fit a regression tree**:\n",
    "   - Train a regression tree $ T_m(X) $ of maximum depth $ d $ on the pseudo-residuals $ r_{m, i} $.\n",
    "   - The tree splits the data into $ J $ terminal regions (leaves) $ R_{m,j} $.\n",
    "\n",
    "3. **Compute leaf values**:\n",
    "   - For each leaf $ R_{m, j} $, compute:\n",
    "     $$\n",
    "     \\gamma_{m,j} = \\frac{\\sum_{X_i \\in R_{m,j}} r_{m, i}}{\\sum_{X_i \\in R_{m,j}} p_{m-1}(X_i)(1 - p_{m-1}(X_i))}\n",
    "     $$\n",
    "     Where:\n",
    "     - $ p_{m-1}(X_i) = \\sigma(F_{m-1}(X_i)) = \\frac{1}{1 + e^{-F_{m-1}(X_i)}} $\n",
    "\n",
    "4. **Update the prediction model**:\n",
    "   - For each sample $ X_i $:\n",
    "     $$\n",
    "     F_m(X_i) = F_{m-1}(X_i) + \\alpha \\cdot \\gamma_{m,j}, \\quad \\text{where } X_i \\in R_{m,j}\n",
    "     $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Final Prediction**\n",
    "1. Compute the final prediction for each sample $ X_i $:\n",
    "   $$\n",
    "   \\hat{p}(X_i) = \\sigma(F_M(X_i)) = \\frac{1}{1 + e^{-F_M(X_i)}}\n",
    "   $$\n",
    "   Where:\n",
    "   $$\n",
    "   F_M(X_i) = F_0(X_i) + \\sum_{m=1}^M \\alpha \\cdot \\gamma_{m,j}, \\quad \\text{where } X_i \\in R_{m,j}\n",
    "   $$\n",
    "\n",
    "2. Classify samples based on the probability threshold (e.g., $ \\hat{p}(X_i) > 0.5 $).\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Notes**\n",
    "1. **Residuals**:\n",
    "   - Residuals $ r_{m,i} $ approximate the gradient of the loss function.\n",
    "   - These residuals are used to fit regression trees in each iteration.\n",
    "\n",
    "2. **Leaf Values ($ \\gamma_{m,j} $)**:\n",
    "   - Computed as the ratio of the sum of residuals to the sum of the product $ p(1-p) $ within each leaf.\n",
    "\n",
    "3. **Learning Rate ($ \\alpha $)**:\n",
    "   - Scales updates to prevent overfitting and stabilize learning.\n",
    "\n",
    "4. **Final Prediction**:\n",
    "   - Probabilities are derived from the final log-odds $ F_M(X_i) $ using the sigmoid function.\n",
    "\n",
    "---\n",
    "\n",
    "This refined pseudo-code covers all essential steps and aligns with your notes. Let me know if you'd like examples or additional clarifications!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
